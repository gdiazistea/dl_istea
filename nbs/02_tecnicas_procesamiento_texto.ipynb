{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Sie0MOwNJi"
      },
      "source": [
        "## Pre-procesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-umnp5oz0Uq"
      },
      "source": [
        "**Normalizaci√≥n:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "57Stgf5ixyhu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT:  La NASA y la ONU lanzaron un cohete pa la Luna. La misi√≥n tuvo un costo de 3,5 millones de d√≥lares. \n",
            "\n",
            "Conversi√≥n de siglas: La Administraci√≥n Nacional de Aeron√°utica y del Espacio y la Organizaci√≥n de las Naciones Unidas lanzaron un cohete pa la Luna. La misi√≥n tuvo un costo de 3,5 millones de d√≥lares. \n",
            "\n",
            "Conversi√≥n a min√∫sculas: la administraci√≥n nacional de aeron√°utica y del espacio y la organizaci√≥n de las naciones unidas lanzaron un cohete pa la luna. la misi√≥n tuvo un costo de 3,5 millones de d√≥lares. \n",
            "\n",
            "Expansi√≥n de contracciones: la administraci√≥n nacional de aeron√°utica y de el espacio y la organizaci√≥n de las naciones unidas lanzaron un cohete para la luna. la misi√≥n tuvo un costo de 3,5 millones de d√≥lares. \n",
            "\n",
            "Normalizaci√≥n de n√∫meros: la administraci√≥n nacional de aeron√°utica y de el espacio y la organizaci√≥n de las naciones unidas lanzaron un cohete para la luna. la misi√≥n tuvo un costo de <n√∫mero>,<n√∫mero> millones de d√≥lares.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Diccionario para la conversi√≥n de siglas en espa√±ol\n",
        "SIGLAS = {\n",
        "    \"ONU\": \"Organizaci√≥n de las Naciones Unidas\",\n",
        "    \"OMS\": \"Organizaci√≥n Mundial de la Salud\",\n",
        "    \"NASA\": \"Administraci√≥n Nacional de Aeron√°utica y del Espacio\",\n",
        "    \"UE\": \"Uni√≥n Europea\"\n",
        "}\n",
        "\n",
        "# Diccionario para la expansi√≥n de contracciones en espa√±ol\n",
        "CONTRACTIONS = {\n",
        "    \"al\": \"a el\",\n",
        "    \"del\": \"de el\",\n",
        "    \"pa\": \"para\",\n",
        "    \"pal\": \"para el\"\n",
        "}\n",
        "\n",
        "\n",
        "# Funci√≥n para convertir a min√∫sculas\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Funci√≥n para normalizar n√∫meros\n",
        "# Util para enfoques de an√°lisis donde los n√∫meros espec√≠ficos no son relevantes o para evitar sesgos en el an√°lisis de texto.\n",
        "def normalize_numbers(text):\n",
        "    # Convertir n√∫meros a su forma escrita o eliminarlos, seg√∫n sea necesario\n",
        "    normalized_text = re.sub(r'\\d+', '<n√∫mero>', text)\n",
        "    return normalized_text\n",
        "\n",
        "# Funci√≥n para expandir contracciones en espa√±ol\n",
        "def expand_contractions(text):\n",
        "    contractions_pattern = re.compile(r'\\b(' + '|'.join(CONTRACTIONS.keys()) + r')\\b', re.IGNORECASE)\n",
        "    expanded_text = contractions_pattern.sub(lambda x: CONTRACTIONS[x.group().lower()], text)\n",
        "    return expanded_text\n",
        "\n",
        "# Funci√≥n para convertir siglas a su forma completa\n",
        "def expand_acronyms(text):\n",
        "    acronyms_pattern = re.compile(r'\\b(' + '|'.join(SIGLAS.keys()) + r')\\b', re.IGNORECASE)\n",
        "    expanded_text = acronyms_pattern.sub(lambda x: SIGLAS[x.group().upper()], text)\n",
        "    return expanded_text\n",
        "\n",
        "# Ejemplo de uso\n",
        "sample_text = \"La NASA y la ONU lanzaron un cohete pa la Luna. La misi√≥n tuvo un costo de 3,5 millones de d√≥lares.\"\n",
        "print(\"INPUT: \", sample_text,\"\\n\")\n",
        "\n",
        "# Conversi√≥n de siglas\n",
        "noacronyms_text = expand_acronyms(sample_text)\n",
        "print(\"Conversi√≥n de siglas:\", noacronyms_text, \"\\n\")\n",
        "\n",
        "# Conversi√≥n a min√∫sculas\n",
        "lowercase_text = to_lowercase(noacronyms_text)\n",
        "print(\"Conversi√≥n a min√∫sculas:\", lowercase_text, \"\\n\")\n",
        "\n",
        "# Expansi√≥n de contracciones\n",
        "expanded_text = expand_contractions(lowercase_text)\n",
        "print(\"Expansi√≥n de contracciones:\", expanded_text, \"\\n\")\n",
        "\n",
        "# Normalizaci√≥n de n√∫meros\n",
        "normalized_text = normalize_numbers(expanded_text)\n",
        "print(\"Normalizaci√≥n de n√∫meros:\", normalized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-chTFZTCz4ef"
      },
      "source": [
        "**Manejo de Emoticones:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsTZKO1kwU3B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ],
      "source": [
        "# %pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HPtW-eb5wP9r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT:  Voy al mercado pal almuerzo. Estoy tan feliz hoy! üòÑ No puedo creerlo, aunque mi amigo est√° triste üò¢. \n",
            "\n",
            "Traducci√≥n de emoticones: Voy al mercado pal almuerzo. Estoy tan feliz hoy! :grinning_face_with_smiling_eyes: No puedo creerlo, aunque mi amigo est√° triste :crying_face:. \n",
            "\n",
            "Eliminaci√≥n de emoticones no relevantes: Voy al mercado pal almuerzo. Estoy tan feliz hoy!  No puedo creerlo, aunque mi amigo est√° triste .\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from emoji import demojize\n",
        "\n",
        "\n",
        "# Funci√≥n para traducir emoticones a texto\n",
        "def translate_emoticons(text):\n",
        "    text = demojize(text)  # Convertir emoticones a sus descripciones textuales\n",
        "    emoticon_translation = {\n",
        "        \":sonriente:\": \"feliz\",\n",
        "        \":triste:\": \"triste\",\n",
        "        \":coraz√≥n:\": \"amor\",\n",
        "        \":pulgar_hacia_arriba:\": \"aprobaci√≥n\",\n",
        "        \":pulgar_hacia_abajo:\": \"desaprobaci√≥n\"\n",
        "    }\n",
        "    for emoticon, meaning in emoticon_translation.items():\n",
        "        text = text.replace(emoticon, meaning)\n",
        "    return text\n",
        "\n",
        "# Funci√≥n para eliminar emoticones no relevantes\n",
        "def remove_irrelevant_emoticons(text):\n",
        "    text = demojize(text)\n",
        "    text = re.sub(r':[^:]+:', '', text)  # Eliminar todas las descripciones de emoticones\n",
        "    return text\n",
        "\n",
        "# Ejemplo de uso\n",
        "sample_text = \"Voy al mercado pal almuerzo. Estoy tan feliz hoy! üòÑ No puedo creerlo, aunque mi amigo est√° triste üò¢.\"\n",
        "print(\"INPUT: \", sample_text,\"\\n\")\n",
        "\n",
        "# Traducci√≥n de emoticones\n",
        "translated_text = translate_emoticons(sample_text)\n",
        "print(\"Traducci√≥n de emoticones:\", translated_text, \"\\n\")\n",
        "\n",
        "# Eliminaci√≥n de emoticones no relevantes (si es necesario)\n",
        "clean_text = remove_irrelevant_emoticons(translated_text)\n",
        "print(\"Eliminaci√≥n de emoticones no relevantes:\", clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhGhNjpx1hAm"
      },
      "source": [
        "**Limpieza de Texto:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulsZruxXN2IF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
            "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
          ]
        }
      ],
      "source": [
        "# %pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2c7UFBCey0s0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT:  \n",
            "Hola,, esttoy aprendiendo procesamiento de lenguaje natural.    Interes√°nte, eh!! \n",
            "\n",
            "Texto sin puntuaci√≥n: \n",
            "Hola esttoy aprendiendo procesamiento de lenguaje natural    Interes√°nte eh \n",
            "\n",
            "Texto sin stop words: Hola esttoy aprendiendo procesamiento lenguaje natural Interes√°nte eh \n",
            "\n",
            "Texto limpio final: Hola esttoy aprendiendo procesamiento lenguaje natural Interes√°nte eh\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar las stop words en espa√±ol\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Inicializaci√≥n\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "# Funci√≥n para eliminar puntuaci√≥n\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Funci√≥n para eliminar stop words\n",
        "def remove_stop_words(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Funci√≥n para eliminar espacios en blanco adicionales\n",
        "def remove_extra_whitespace(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Ejemplo de uso\n",
        "sample_text = \"\\nHola,, esttoy aprendiendo procesamiento de lenguaje natural.    Interes√°nte, eh!!\"\n",
        "print(\"INPUT: \", sample_text,\"\\n\")\n",
        "\n",
        "# 1. Eliminaci√≥n de Puntuaci√≥n\n",
        "text_no_punct = remove_punctuation(sample_text)\n",
        "print(\"Texto sin puntuaci√≥n:\", text_no_punct, \"\\n\")\n",
        "\n",
        "# 2. Eliminaci√≥n de Stop Words\n",
        "text_no_stop_words = remove_stop_words(text_no_punct)\n",
        "print(\"Texto sin stop words:\", text_no_stop_words, \"\\n\")\n",
        "\n",
        "# 3. Eliminaci√≥n de Espacios en Blanco Adicionales\n",
        "clean_text = remove_extra_whitespace(text_no_stop_words)\n",
        "print(\"Texto limpio final:\", clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa8w31jusQ0p"
      },
      "source": [
        "**ADICIONAL**: investigar librer√≠as para correcci√≥n ortogr√°fica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e_3Q0GxeeQj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n",
            "Texto con correcci√≥n ortogr√°fica: Hola esto aprendiente procesamiento lenguaje natural Interes√°nte eh\n"
          ]
        }
      ],
      "source": [
        "# %pip install pyspellchecker\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker(language='es')\n",
        "\n",
        "def correct_spelling(text):\n",
        "  corrected_text = []\n",
        "  misspelled = spell.unknown(text.split())\n",
        "  for word in text.split():\n",
        "    if word in misspelled:\n",
        "      corrected_word = spell.correction(word)\n",
        "      corrected_text.append(corrected_word)\n",
        "    else:\n",
        "      corrected_text.append(word)\n",
        "  return \" \".join(corrected_text)\n",
        "\n",
        "corrected_text = correct_spelling(clean_text)\n",
        "print(\"Texto con correcci√≥n ortogr√°fica:\", corrected_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z56FTcbzhcp"
      },
      "source": [
        "## Tokenizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTY57-JB5_8U"
      },
      "source": [
        "**Librer√≠a NLTK:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ev6NoevCNPPE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import NLTK and download required data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "74b7eJ74NvzJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Definici√≥n de NLTK:\n",
            "\n",
            "NTLK (Natural Language Toolkit) es una biblioteca de procesamiento de lenguaje natural en Python.\n",
            "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenizaci√≥n, an√°lisis gramatical,\n",
            "etiquetado de partes del discurso, entre otros.\n",
            "\n",
            "\n",
            "Ejemplo de Tokenizaci√≥n:\n",
            "Oraci√≥n original: Apple est√° buscando comprar una startup del Reino Unido por mil millones de d√≥lares.\n",
            "Tokens: ['Apple', 'est√°', 'buscando', 'comprar', 'una', 'startup', 'del', 'Reino', 'Unido', 'por', 'mil', 'millones', 'de', 'd√≥lares', '.']\n"
          ]
        }
      ],
      "source": [
        "# Define NLTK\n",
        "nltk_definition = \"\"\"\n",
        "NTLK (Natural Language Toolkit) es una biblioteca de procesamiento de lenguaje natural en Python.\n",
        "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenizaci√≥n, an√°lisis gramatical,\n",
        "etiquetado de partes del discurso, entre otros.\n",
        "\"\"\"\n",
        "\n",
        "# Print definition\n",
        "print(\"\\nDefinici√≥n de NLTK:\")\n",
        "print(nltk_definition)\n",
        "\n",
        "# Tokenization example\n",
        "sentence = \"Apple est√° buscando comprar una startup del Reino Unido por mil millones de d√≥lares.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nEjemplo de Tokenizaci√≥n:\")\n",
        "print(f\"Oraci√≥n original: {sentence}\")\n",
        "print(f\"Tokens: {tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fDzQRIXDjILw"
      },
      "outputs": [],
      "source": [
        "cuento = \"En una noche tormentosa, Clara escuch√≥ pasos en el √°tico, aunque viv√≠a sola. \\\n",
        "  Subi√≥ las escaleras temblorosa, y al abrir la puerta, encontr√≥ una mu√±eca antigua mir√°ndola fijamente. \\\n",
        "  De repente, la mu√±eca sonri√≥ y susurr√≥: 'Te estaba esperando.'\"\n",
        "\n",
        "# EJERCICIO: tokenizar palabras, frases, mostrar cantidad de tokens de ambas categor√≠as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSXREfIVfi4"
      },
      "source": [
        "**Librer√≠a SpaCy:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18z9xliKNItL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
            "Collecting jinja2 (from spacy)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting setuptools (from spacy)\n",
            "  Downloading setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from spacy) (24.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.33.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-extensions>=4.12.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
            "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.8/31.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
            "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "Downloading numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
            "Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
            "Downloading pydantic_core-2.33.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Downloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (145 kB)\n",
            "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Downloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
            "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-extensions, spacy-loggers, spacy-legacy, shellingham, setuptools, numpy, murmurhash, mdurl, MarkupSafe, idna, cloudpathlib, charset-normalizer, certifi, catalogue, annotated-types, typing-inspection, srsly, smart-open, requests, pydantic-core, preshed, markdown-it-py, marisa-trie, jinja2, blis, rich, pydantic, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
            "Successfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 certifi-2025.1.31 charset-normalizer-3.4.1 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 idna-3.10 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 numpy-2.2.4 preshed-3.0.9 pydantic-2.11.1 pydantic-core-2.33.0 requests-2.32.3 rich-13.9.4 setuptools-78.1.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 typing-extensions-4.13.0 typing-inspection-0.4.0 urllib3-2.3.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
          ]
        }
      ],
      "source": [
        "# %pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNTlRtt5Vh1M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "# Descarga un modelo de lenguaje para el espa√±ol en la biblioteca de PLN SpaCy,\n",
        "# optimizado para noticias (\"core_news\") y es de tama√±o peque√±o (\"sm\" para \"small\")\n",
        "# !python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HkE0_zUaVhse"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Definici√≥n de SpaCy:\n",
            "\n",
            "SpaCy es una biblioteca de procesamiento de lenguaje natural en Python, optimizada para rendimiento.\n",
            "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenizaci√≥n, etiquetado de entidades,\n",
            "an√°lisis sint√°ctico, entre otros.\n",
            "\n",
            "Modelo cargado: core_news_sm\n",
            "\n",
            "Ejemplo de Tokenizaci√≥n con SpaCy:\n",
            "Oraci√≥n original: Apple est√° buscando comprar una startup del Reino Unido por mil millones de d√≥lares.\n",
            "Tokens: ['Apple', 'est√°', 'buscando', 'comprar', 'una', 'startup', 'del', 'Reino', 'Unido', 'por', 'mil', 'millones', 'de', 'd√≥lares', '.']\n",
            "\n",
            "Ventajas de SpaCy:\n",
            "- R√°pido: SpaCy est√° optimizado para un rendimiento r√°pido, lo que lo hace adecuado para aplicaciones en tiempo real.\n",
            "- Extensible: SpaCy permite la creaci√≥n de pipelines personalizadas y la integraci√≥n con otras bibliotecas.\n",
            "- Soporte para m√∫ltiples idiomas: SpaCy proporciona modelos para varios idiomas, facilitando el procesamiento de texto multiling√ºe.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import SpaCy\n",
        "import spacy\n",
        "\n",
        "# Define SpaCy\n",
        "spacy_definition = \"\"\"\n",
        "SpaCy es una biblioteca de procesamiento de lenguaje natural en Python, optimizada para rendimiento.\n",
        "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenizaci√≥n, etiquetado de entidades,\n",
        "an√°lisis sint√°ctico, entre otros.\n",
        "\"\"\"\n",
        "\n",
        "# Print SpaCy definition\n",
        "print(\"\\nDefinici√≥n de SpaCy:\")\n",
        "print(spacy_definition)\n",
        "\n",
        "# model loading\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "print(\"Modelo cargado:\", nlp.meta[\"name\"])\n",
        "\n",
        "# Frase de prueba\n",
        "doc = nlp(\"Apple est√° buscando comprar una startup del Reino Unido por mil millones de d√≥lares.\")\n",
        "\n",
        "# Print SpaCy results\n",
        "print(\"\\nEjemplo de Tokenizaci√≥n con SpaCy:\")\n",
        "print(f\"Oraci√≥n original: {doc.text}\")\n",
        "print(\"Tokens:\", [token.text for token in doc])\n",
        "\n",
        "# Advantages of SpaCy\n",
        "spacy_advantages = \"\"\"\n",
        "Ventajas de SpaCy:\n",
        "- R√°pido: SpaCy est√° optimizado para un rendimiento r√°pido, lo que lo hace adecuado para aplicaciones en tiempo real.\n",
        "- Extensible: SpaCy permite la creaci√≥n de pipelines personalizadas y la integraci√≥n con otras bibliotecas.\n",
        "- Soporte para m√∫ltiples idiomas: SpaCy proporciona modelos para varios idiomas, facilitando el procesamiento de texto multiling√ºe.\n",
        "\"\"\"\n",
        "print(spacy_advantages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ei-dtmvHjwv"
      },
      "source": [
        "## Lematizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntv42MrM8hn4"
      },
      "source": [
        "El lema es la forma base o ra√≠z de una palabra. Por ejemplo, \"comprar\" es el lema de \"comprando\".\n",
        "\n",
        "La lematizaci√≥n permite normalizar diferentes formas de una palabra a su forma b√°sica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qNnEXY419DVj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabra: corriendo, Lema: correr\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Cargar el modelo en espa√±ol de spaCy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto = \"corriendo\"\n",
        "\n",
        "# Procesar el texto con spaCy\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Extraer y mostrar el lema\n",
        "for token in doc:\n",
        "    print(f\"Palabra: {token.text}, Lema: {token.lemma_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLASRVG4B8w0"
      },
      "source": [
        "## Radicalizaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JLXDK33-F1XA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras original: ['corriendo', 'corredor', 'corr√≠', 'cocinero', 'cocina', 'cocinada', 'nacionalidad']\n",
            "Radicalizaci√≥n Porter  : ['corriendo', 'corredor', 'corr√≠', 'cocinero', 'cocina', 'cocinada', 'nacionalidad']\n",
            "Radicalizaci√≥n Snowball: ['corr', 'corredor', 'corr', 'cociner', 'cocin', 'cocin', 'nacional']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Ejemplos de palabras para radicalizar\n",
        "words = ['corriendo', 'corredor', 'corr√≠', 'cocinero', 'cocina', 'cocinada', 'nacionalidad']\n",
        "\n",
        "# Porter Stemmer\n",
        "stemmer = PorterStemmer() # only English!!!\n",
        "# Aplicar el Porter Stemmer\n",
        "stemmed_words_porter = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Instanciar el Snowball Stemmer para espa√±ol\n",
        "stemmer = SnowballStemmer('spanish') # Snowball: extensi√≥n del Porter para multilenguaje\n",
        "# Aplicar el Snowball Stemmer\n",
        "stemmed_words_snowball = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Palabras original:\", words)\n",
        "print(\"Radicalizaci√≥n Porter  :\", stemmed_words_porter)\n",
        "print(\"Radicalizaci√≥n Snowball:\", stemmed_words_snowball)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gtRVf1HMjc5"
      },
      "source": [
        "¬øQu√© podemos analizar de estos resultados?\n",
        "\n",
        "Veamos ahora un ejemplo en ingl√©s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3fuC39BpHd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras original: ['running', 'runner', 'run', 'cooking', 'cookbook', 'cookery', 'generalization']\n",
            "Radicalizaci√≥n Porter  : ['run', 'runner', 'run', 'cook', 'cookbook', 'cookeri', 'gener']\n",
            "Radicalizaci√≥n Snowball: ['run', 'runner', 'run', 'cook', 'cookbook', 'cookeri', 'general']\n"
          ]
        }
      ],
      "source": [
        "# ENGLISH example\n",
        "\n",
        "# Ejemplos de palabras para radicalizar\n",
        "words = ['running', 'runner', 'run', 'cooking', 'cookbook', 'cookery', 'generalization']\n",
        "\n",
        "# Porter Stemmer\n",
        "stemmer = PorterStemmer() # only English!!!\n",
        "# Aplicar el Porter Stemmer\n",
        "stemmed_words_porter = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Instanciar el Snowball Stemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "# Aplicar el Snowball Stemmer\n",
        "stemmed_words_snowball = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Palabras original:\", words)\n",
        "print(\"Radicalizaci√≥n Porter  :\", stemmed_words_porter)\n",
        "print(\"Radicalizaci√≥n Snowball:\", stemmed_words_snowball)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwdR6uu9H02d"
      },
      "source": [
        "## Etiquetado de Partes del Discurso (POS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E9vtAw4B6ea"
      },
      "source": [
        "La etiqueta de parte del discurso (POS) indica la categor√≠a gramatical de la palabra, algunas de las cuales son:\n",
        "- PROPN: Nombre propio.\n",
        "- AUX: Verbo auxiliar.\n",
        "- VERB: Verbo.\n",
        "- DET: Determinante.\n",
        "- NOUN: Sustantivo.\n",
        "- ADP: Preposici√≥n.\n",
        "- NUM: N√∫mero.\n",
        "- PUNCT: Signo de puntuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SCfgL5weG8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Etiquetado POS del SpaCy:\n",
            "\n",
            "El: DET\n",
            "gato: NOUN\n",
            "est√°: AUX\n",
            "debajo: ADV\n",
            "de: ADP\n",
            "la: DET\n",
            "mesa: NOUN\n",
            ".: PUNCT\n"
          ]
        }
      ],
      "source": [
        "# 1): usando spaCy\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Cargar el modelo en espa√±ol\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Procesar el texto\n",
        "text = \"El gato est√° debajo de la mesa.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Mostrar las etiquetas de partes del discurso\n",
        "print(\"Etiquetado POS del SpaCy:\\n\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-p2zkE9AwG5P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oraciones tokenizadas:\n",
            "['El gato Felipe est√° en el tejado.', 'Parece que va a saltar.']\n",
            "\n",
            "Palabras tokenizadas:\n",
            "['El', 'gato', 'Felipe', 'est√°', 'en', 'el', 'tejado', '.', 'Parece', 'que', 'va', 'a', 'saltar', '.'] \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package cess_esp is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/gonzadzz/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/share/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(palabras,\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Etiquetar partes del discurso\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m tagged = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpalabras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspa\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Change 'tokens' to 'palabras' to use the correct variable\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEtiquetado POS del NLTK:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(tagged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m    275\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.weights = json.load(fin)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/gonzadzz/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/share/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# 2): usando NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import cess_esp\n",
        "\n",
        "# Descargar los datos necesarios\n",
        "nltk.download('cess_esp')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') # modelo pre-entrenado s√≥lo para English!!!\n",
        "\n",
        "# TEXTO\n",
        "text = \"El gato Felipe est√° en el tejado. Parece que va a saltar.\"\n",
        "\n",
        "# Tokenizaci√≥n de oraciones\n",
        "oraciones = sent_tokenize(text, language='spanish')\n",
        "print(\"Oraciones tokenizadas:\")\n",
        "print(oraciones)\n",
        "\n",
        "# Tokenizaci√≥n de palabras\n",
        "palabras = word_tokenize(text, language='spanish') # The tokenized words are stored in the variable 'palabras'\n",
        "print(\"\\nPalabras tokenizadas:\")\n",
        "print(palabras,\"\\n\")\n",
        "\n",
        "# Etiquetar partes del discurso\n",
        "tagged = nltk.pos_tag(palabras, lang='spa') # Change 'tokens' to 'palabras' to use the correct variable\n",
        "print(\"Etiquetado POS del NLTK:\\n\")\n",
        "print(tagged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfth2RvGwCbB"
      },
      "source": [
        "Podemos ver que el espa√±ol no est√° soportado todav√≠a...\n",
        "\n",
        "Probemos otra aproximaci√≥n: entrenar un etiquetador POS en espa√±ol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UFTwU0o3v_zh"
      },
      "outputs": [],
      "source": [
        "# Cargar el corpus de oraciones etiquetadas\n",
        "oraciones = cess_esp.tagged_sents()\n",
        "\n",
        "# Entrenar un etiquetador basado en unigramas y bigramas\n",
        "from nltk.tag import UnigramTagger, BigramTagger\n",
        "\n",
        "unigram_tagger = UnigramTagger(oraciones)\n",
        "bigram_tagger = BigramTagger(oraciones, backoff=unigram_tagger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fwe-iiFlkhPV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricit√©_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunci√≥', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_√Åguila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japon√©s', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')], [('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explic√≥', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcci√≥n', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prev√©', 'vmm02s0'), ('la', 'da0fs0'), ('utilizaci√≥n', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')], ...]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oraciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ESpXwIPSkp-E"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6030"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(oraciones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "X-TegOezxDT7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('El', 'da0ms0'), ('gato', 'ncms000'), ('Felipe', 'np0000p'), ('est√°', 'vmip3s0'), ('en', 'sps00'), ('el', 'da0ms0'), ('tejado', None), ('.', 'Fp'), ('Parece', 'vmip3s0'), ('que', 'cs'), ('va', 'vmip3s0'), ('a', 'sps00'), ('saltar', 'vmn0000'), ('.', 'Fp')]\n"
          ]
        }
      ],
      "source": [
        "# Prueba del etiquetador\n",
        "text = \"El gato Felipe est√° en el tejado. Parece que va a saltar.\"\n",
        "tokens = nltk.word_tokenize(text, language='spanish')\n",
        "etiquetas = bigram_tagger.tag(tokens)\n",
        "print(etiquetas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DEXrpXnxYYN"
      },
      "source": [
        "El corpus cess_esp contiene oraciones en espa√±ol con sus respectivas etiquetas POS.\n",
        "\n",
        "Las etiquetas morfosint√°cticas est√°n en el formato EAGLES (Expert Advisory Group on Language Engineering Standards), un est√°ndar ampliamente usado en ling√º√≠stica computacional para la anotaci√≥n gramatical.\n",
        "\n",
        "Cada etiqueta es un c√≥digo alfanum√©rico que describe la categor√≠a gramatical de la palabra y algunas de sus propiedades morfosint√°cticas:\n",
        "\n",
        "Primera letra: Categor√≠a gramatical principal.\n",
        "\n",
        "- n = Sustantivo (noun)\n",
        "- v = Verbo (verb)\n",
        "- a = Adjetivo (adjective)\n",
        "- d = Determinante (determiner)\n",
        "- s = Preposici√≥n o contracci√≥n (preposition)\n",
        "- c = Conjunci√≥n (conjunction)\n",
        "- p = Pronombre (pronoun)\n",
        "- f = Signo de puntuaci√≥n (punctuation)\n",
        "\n",
        "Segunda letra: Tipo o subtipo de la categor√≠a.\n",
        "\n",
        "- Para sustantivos (n), indica si es com√∫n (c) o propio (p).\n",
        "- Para verbos (v), se refiere al modo (indicativo, subjuntivo, etc.).\n",
        "\n",
        "Tercera letra y siguientes:\n",
        "- Caracter√≠sticas adicionales como g√©nero, n√∫mero, tiempo verbal, etc.\n",
        "\n",
        "\n",
        "**Consideraciones**:\n",
        "\n",
        "El etiquetador entrenado con el corpus CESS-ESP puede no ser tan preciso para textos fuera de su dominio.\n",
        "\n",
        "Para un etiquetado POS m√°s robusto en espa√±ol, se recomiendan herramientas como spaCy o Stanza, que tienen modelos entrenados espec√≠ficamente para espa√±ol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LBxwYCorHDFP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Kubernetes', None), ('sirve', 'vmip3s0'), ('para', 'sps00'), ('automatizar', None), ('la', 'da0fs0'), ('orquestaci√≥n', None), ('de', 'sps00'), ('contenedores', 'ncmp000'), ('.', 'Fp')]\n"
          ]
        }
      ],
      "source": [
        "# Frase fuera del contexto de entrenamiento\n",
        "text = \"Kubernetes sirve para automatizar la orquestaci√≥n de contenedores.\"\n",
        "tokens = nltk.word_tokenize(text, language='spanish')\n",
        "etiquetas = bigram_tagger.tag(tokens)\n",
        "print(etiquetas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udydAe4qH8-_"
      },
      "source": [
        "Veamos un ejemplo en ingl√©s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FYktpq3LIFcg"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/gonzadzz/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/share/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m tokens = word_tokenize(text, language=\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Mostrar las etiquetas de partes del discurso usando Penn Treebank\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tagged = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meng\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(tagged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m    275\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.weights = json.load(fin)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/gonzadzz/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/share/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "text = \"Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services.\"\n",
        "tokens = word_tokenize(text, language='english')\n",
        "\n",
        "# Mostrar las etiquetas de partes del discurso usando Penn Treebank\n",
        "tagged = nltk.pos_tag(tokens, lang='eng')\n",
        "print(tagged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_lwtibFIiMA"
      },
      "source": [
        "Vemos que los c√≥digos son diferentes. Aqu√≠ el NLTK usa el conjunto Penn Treebank, que es uno de los m√°s comunes para etiquetado POS en ingl√©s.\n",
        "\n",
        "Explicaci√≥n de algunos c√≥digos:\n",
        "- NNS: Sustantivo en plural (e.g., \"workloads\", \"services\").\n",
        "- VBZ: Verbo en tercera persona del singular en presente (e.g., \"is\").\n",
        "- DT: Determinante o art√≠culo (e.g., \"a\").\n",
        "- JJ: Adjetivo (e.g., \"portable\", \"extensible\", \"containerized\", \"open\").\n",
        "- ,: Signo de puntuaci√≥n (coma).\n",
        "- NN: Sustantivo en singular (e.g., \"source\", \"platform\").\n",
        "- IN: Preposici√≥n o conjunci√≥n subordinante (e.g., \"for\").\n",
        "- VBG: Verbo en gerundio o participio presente (e.g., \"managing\").\n",
        "- CC: Conjunci√≥n de coordinaci√≥n (e.g., \"and\").\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAOrs-pEHEu8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textBlob\n",
            "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: nltk>=3.9 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from textBlob) (3.9.1)\n",
            "Requirement already satisfied: click in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (4.67.1)\n",
            "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: textBlob\n",
            "Successfully installed textBlob-0.19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/gonzadzz/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextBlob POS Tagging:\n",
            "El: NNP\n",
            "gato: NN\n",
            "est√°: NN\n",
            "debajo: NN\n",
            "de: IN\n",
            "la: FW\n",
            "mesa: FW\n"
          ]
        }
      ],
      "source": [
        "# 3): usando TextBlob\n",
        "# %pip install textBlob\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob.download_corpora import download_all\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "download_all()\n",
        "\n",
        "# Crear un objeto TextBlob\n",
        "text = \"El gato est√° debajo de la mesa.\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Etiquetar partes del discurso\n",
        "print(\"TextBlob POS Tagging:\")\n",
        "for word, tag in blob.tags:\n",
        "    print(f\"{word}: {tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1slgD-UqNyia"
      },
      "source": [
        "¬øQu√© tal el resultado...?\n",
        "\n",
        "Veamos ahora un ejemplo en ingl√©s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SULbMJQ91qkx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextBlob POS Tagging:\n",
            "The: DT\n",
            "cat: NN\n",
            "is: VBZ\n",
            "under: IN\n",
            "the: DT\n",
            "table: NN\n"
          ]
        }
      ],
      "source": [
        "# English example\n",
        "\n",
        "# Tokenizar el texto\n",
        "text = \"The cat is under the table.\"\n",
        "\n",
        "# Mostrar las etiquetas de partes del discurso\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Etiquetar partes del discurso\n",
        "print(\"TextBlob POS Tagging:\")\n",
        "for word, tag in blob.tags:\n",
        "    print(f\"{word}: {tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yEL-6zeHNkp"
      },
      "source": [
        "NOTAS:\n",
        "\n",
        "- NLTK: Ofrece flexibilidad y permite personalizar modelos, pero requiere configuraciones adicionales y puede no ser tan preciso en el espa√±ol.\n",
        "\n",
        "- SpaCy: Proporciona modelos preentrenados precisos y f√°ciles de usar para m√∫ltiples idiomas.\n",
        "\n",
        "- TextBlob: Es una opci√≥n m√°s sencilla y ligera, pero con soporte limitado para idiomas distintos del ingl√©s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldjReL1-TsDW"
      },
      "source": [
        "#### Un poco m√°s de informaci√≥n!\n",
        "\n",
        "*Dependencia (Dependency Parsing)*:\n",
        "\n",
        "La dependencia describe la relaci√≥n sint√°ctica entre la palabra y su palabra \"padre\" (head). Las etiquetas de dependencia indican el papel gramatical de la palabra en la oraci√≥n. Algunos ejemplos comunes son:\n",
        "- nsubj: Sujeto nominal.\n",
        "- aux: Auxiliar.\n",
        "- ROOT: Ra√≠z de la oraci√≥n.\n",
        "- xcomp: Complemento de objeto abierto.\n",
        "- det: Determinante.\n",
        "- obj: Objeto.\n",
        "- case: Preposici√≥n o palabra relacionada con la marca de caso.\n",
        "- nmod: Modificador nominal.\n",
        "- flat: Dependencia plana (usualmente para nombres propios compuestos).\n",
        "- nummod: Modificador num√©rico.\n",
        "- obl: Objeto oblicuo.\n",
        "- nmod: Modificador nominal.\n",
        "- punct: Puntuaci√≥n.\n",
        "\n",
        "*Padre (Head)*:\n",
        "\n",
        "La palabra padre es el token principal al que est√° relacionado un token dado en la estructura sint√°ctica de la oraci√≥n. En un √°rbol de dependencias, cada palabra est√° conectada a otra palabra (su padre) hasta llegar a la ra√≠z de la oraci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "s9Fr7js6UMml"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "An√°lisis Sint√°ctico:\n",
            "Texto: El, Lema: el, POS: DET, Dependencia: det, Padre: gato\n",
            "Texto: gato, Lema: gato, POS: NOUN, Dependencia: nsubj, Padre: debajo\n",
            "Texto: est√°, Lema: estar, POS: AUX, Dependencia: cop, Padre: debajo\n",
            "Texto: debajo, Lema: debajo, POS: ADV, Dependencia: ROOT, Padre: debajo\n",
            "Texto: de, Lema: de, POS: ADP, Dependencia: case, Padre: mesa\n",
            "Texto: la, Lema: el, POS: DET, Dependencia: det, Padre: mesa\n",
            "Texto: mesa, Lema: mesa, POS: NOUN, Dependencia: obl, Padre: debajo\n",
            "Texto: ., Lema: ., POS: PUNCT, Dependencia: punct, Padre: debajo\n"
          ]
        }
      ],
      "source": [
        "# imprimir el an√°lisis sint√°ctico, una entidad por l√≠nea\n",
        "print(\"\\nAn√°lisis Sint√°ctico:\")\n",
        "for token in doc:\n",
        "    print(f\"Texto: {token.text}, Lema: {token.lemma_}, POS: {token.pos_}, Dependencia: {token.dep_}, Padre: {token.head.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "kNzNO5mvxmIe"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m displacy\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Visualizar el an√°lisis sint√°ctico\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdep\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfont\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAreal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdistance\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcompact\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjupyter\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/spacy/displacy/__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)"
          ]
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# Visualizar el an√°lisis sint√°ctico\n",
        "displacy.render(doc,style = 'dep', options = {'font':'Areal','distance':100,'compact' : True,}, jupyter =True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou8yQK7HLVNx"
      },
      "source": [
        "## Reconocimiento de entidades nombradas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_71JPNlp1pYW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting es-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.8.0/es_core_news_lg-3.8.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-lg\n",
            "Successfully installed es-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n"
          ]
        }
      ],
      "source": [
        "# Modelo de lenguaje para el espa√±ol en la biblioteca de PLN SpaCy,\n",
        "# optimizado para noticias (\"core_news\") y es de tama√±o grande (\"lg\")\n",
        "!python -m spacy download es_core_news_lg #md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ1EP2J5Yp59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (24.2)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
            "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Installing collected packages: safetensors, pyyaml, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed filelock-3.18.0 fsspec-2025.3.0 huggingface-hub-0.29.3 pyyaml-6.0.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entidades Nombradas con SpaCy --espa√±ol--:\n",
            "[('Ra√∫l Alfons√≠n', 'PER'), ('Buenos Aires', 'LOC'), ('Argentina', 'LOC')]\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEntidades Nombradas con SpaCy --espa√±ol--:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(entidades_spacy)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_spacy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m texto = \u001b[33m\"\u001b[39m\u001b[33mRa√∫l Alfons√≠n, a lawyer born in Buenos Aires, was the President of Argentina between 1983 and 1989.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m nlp_spacy = spacy.load(\u001b[33m\"\u001b[39m\u001b[33men_core_web_sm\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/spacy/displacy/__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)"
          ]
        }
      ],
      "source": [
        "%pip install transformers \n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from textblob import TextBlob\n",
        "from transformers import pipeline\n",
        "\n",
        "# Cargar el modelo de SpaCy para espa√±ol\n",
        "nlp_spacy = spacy.load(\"es_core_news_lg\")\n",
        "\n",
        "# Texto a analizar\n",
        "texto = \"Ra√∫l Alfons√≠n, abogado nacido en Buenos Aires, fue presidente de Argentina, entre 1983 y 1989.\"\n",
        "\n",
        "# 1. Reconocimiento de Entidades Nombradas con SpaCy\n",
        "doc_spacy = nlp_spacy(texto)\n",
        "entidades_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents]\n",
        "print(\"\\nEntidades Nombradas con SpaCy --espa√±ol--:\")\n",
        "print(entidades_spacy)\n",
        "displacy.render(doc_spacy, style=\"ent\")\n",
        "\n",
        "\n",
        "texto = \"Ra√∫l Alfons√≠n, a lawyer born in Buenos Aires, was the President of Argentina between 1983 and 1989.\"\n",
        "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "doc_spacy = nlp_spacy(texto)\n",
        "entidades_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents]\n",
        "print(\"\\nEntidades Nombradas con SpaCy --ingl√©s--:\")\n",
        "print(entidades_spacy)\n",
        "displacy.render(doc_spacy, style=\"ent\")\n",
        "\n",
        "\n",
        "# 2. Reconocimiento de Entidades Nombradas con TextBlob\n",
        "# TextBlob no tiene un modelo preentrenado para NER en espa√±ol, por lo que no es adecuado para este prop√≥sito\n",
        "# Para demostrar el uso de TextBlob en ingl√©s, el siguiente c√≥digo ser√≠a un ejemplo:\n",
        "blob = TextBlob(\"Ra√∫l Alfons√≠n, a lawyer born in Buenos Aires, was the President of Argentina between 1983 and 1989.\")\n",
        "entidades_textblob = blob.noun_phrases\n",
        "print(\"\\nEntidades Nombradas con TextBlob (Ingl√©s):\")\n",
        "print(entidades_textblob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVxufJNUSpX9"
      },
      "source": [
        "¬øQu√© diferencias se pueden apreciar en los resultados?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMXlOMknLwOO"
      },
      "source": [
        "## Ejercicio: An√°lisis de Texto en Espa√±ol con NLTK y SpaCy\n",
        "\n",
        "Objetivos:\n",
        "- Aprender a tokenizar texto utilizando NLTK y SpaCy.\n",
        "- Comparar los resultados de tokenizaci√≥n entre ambas librer√≠as.\n",
        "- Utilizar SpaCy para realizar el reconocimiento de entidades nombradas.\n",
        "- Realizar el an√°lisis sint√°ctico de una oraci√≥n con SpaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "H6da4WCpMDLF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparaci√≥n de tokenizaciones:\n",
            "NTLK : ['El', 'presidente', 'Barack', 'Obama', 'visit√≥', 'Par√≠s', 'en', 'julio', 'de', '2023', 'para', 'una', 'cumbre', 'internacional', '.']\n",
            "SpaCy: ['El', 'presidente', 'Barack', 'Obama', 'visit√≥', 'Par√≠s', 'en', 'julio', 'de', '2023', 'para', 'una', 'cumbre', 'internacional', '.']\n",
            "\n",
            "Entidades Nombradas con SpaCy:\n",
            "[('Barack Obama visit√≥ Par√≠s', 'PER')]\n",
            "\n",
            "An√°lisis sint√°ctico completo con SpaCy:\n",
            "Texto: El, Lema: el, POS: DET, Dependencia: det, Padre: presidente\n",
            "Texto: presidente, Lema: presidente, POS: NOUN, Dependencia: nsubj, Padre: visit√≥\n",
            "Texto: Barack, Lema: Barack, POS: PROPN, Dependencia: appos, Padre: presidente\n",
            "Texto: Obama, Lema: Obama, POS: PROPN, Dependencia: flat, Padre: Barack\n",
            "Texto: visit√≥, Lema: visitar, POS: VERB, Dependencia: ROOT, Padre: visit√≥\n",
            "Texto: Par√≠s, Lema: Par√≠s, POS: PROPN, Dependencia: obj, Padre: visit√≥\n",
            "Texto: en, Lema: en, POS: ADP, Dependencia: case, Padre: julio\n",
            "Texto: julio, Lema: julio, POS: NOUN, Dependencia: obl, Padre: visit√≥\n",
            "Texto: de, Lema: de, POS: ADP, Dependencia: case, Padre: 2023\n",
            "Texto: 2023, Lema: 2023, POS: NUM, Dependencia: compound, Padre: julio\n",
            "Texto: para, Lema: para, POS: ADP, Dependencia: case, Padre: cumbre\n",
            "Texto: una, Lema: uno, POS: DET, Dependencia: det, Padre: cumbre\n",
            "Texto: cumbre, Lema: cumbre, POS: NOUN, Dependencia: obl, Padre: visit√≥\n",
            "Texto: internacional, Lema: internacional, POS: ADJ, Dependencia: amod, Padre: cumbre\n",
            "Texto: ., Lema: ., POS: PUNCT, Dependencia: punct, Padre: visit√≥\n"
          ]
        }
      ],
      "source": [
        "# Texto a analizar\n",
        "texto = \"El presidente Barack Obama visit√≥ Par√≠s en julio de 2023 para una cumbre internacional.\"\n",
        "#texto = \"El Dr. Juan P√©rez, un reconocido neurocirujano de Nueva York, intervino exitosamente a la Sra. Julia Gonz√°lez.\"\n",
        "\n",
        "# Tokenizaci√≥n con NLTK\n",
        "tokens_nltk = nltk.word_tokenize(texto)\n",
        "\n",
        "# Tokenizaci√≥n con SpaCy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "doc = nlp(texto)\n",
        "tokens_spacy = [token.text for token in doc]\n",
        "\n",
        "# Comparaci√≥n de Tokenizaci√≥n\n",
        "print(\"\\nComparaci√≥n de tokenizaciones:\")\n",
        "print(\"NTLK :\", tokens_nltk)\n",
        "print(\"SpaCy:\", tokens_spacy)\n",
        "\n",
        "\n",
        "# Reconocimiento de Entidades Nombradas con SpaCy\n",
        "entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"\\nEntidades Nombradas con SpaCy:\")\n",
        "print(entidades)\n",
        "\n",
        "# An√°lisis Sint√°ctico con SpaCy\n",
        "print(\"\\nAn√°lisis sint√°ctico completo con SpaCy:\")\n",
        "for token in doc:\n",
        "    print(f\"Texto: {token.text}, Lema: {token.lemma_}, POS: {token.pos_}, Dependencia: {token.dep_}, Padre: {token.head.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTp-M8gzMGGP"
      },
      "source": [
        "Discusi√≥n para los resultados con ambras frases:\n",
        "\n",
        "- ¬øQu√© diferencias notaste en la tokenizaci√≥n entre NLTK y SpaCy?\n",
        "- ¬øQu√© ventajas ofrece SpaCy en t√©rminos de rendimiento y capacidades adicionales?\n",
        "\n",
        "Discusi√≥n general:\n",
        "\n",
        "- ¬øC√≥mo puede el reconocimiento de entidades nombradas ayudar en el an√°lisis de texto?\n",
        "- ¬øQu√© informaci√≥n √∫til puedes obtener del an√°lisis sint√°ctico realizado con SpaCy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcdiSK1WSziR"
      },
      "source": [
        "1) La tokenizaci√≥n de SpaCy es m√°s precisa y maneja mejor los signos de puntuaci√≥n y las entidades compuestas en comparaci√≥n con NLTK.\n",
        "\n",
        "2) SpaCy ofrece un rendimiento superior y capacidades adicionales como el reconocimiento de entidades nombradas y el an√°lisis sint√°ctico, haci√©ndolo m√°s adecuado para aplicaciones en tiempo real.\n",
        "\n",
        "3) El reconocimiento de entidades nombradas facilita la identificaci√≥n de elementos clave en el texto, como nombres de personas, lugares y organizaciones, mejorando la comprensi√≥n y el an√°lisis contextual.\n",
        "\n",
        "4) El an√°lisis sint√°ctico con SpaCy proporciona informaci√≥n detallada sobre las relaciones gramaticales entre las palabras, lo que ayuda a entender la estructura y el significado subyacente de las oraciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuQh6_3OmiZC"
      },
      "source": [
        "#### Ejemplo de an√°lisis de sentimiento usando NLTK\n",
        "\n",
        "Fuente:\n",
        "https://www.datacamp.com/es/tutorial/text-analytics-beginners-nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU-Kt3ZImsNu"
      },
      "outputs": [],
      "source": [
        "# Start codiimport pandas as pd \n",
        "%pip install pandas\n",
        "%pip install pandas\n",
        " \n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# download nltk corpus\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "# Load the amazon review dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/amazon.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s83nexRNpUHU"
      },
      "outputs": [],
      "source": [
        "# imprimir 5 filas con columna Positive igual a 0\n",
        "df[df['Positive'] == 0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAFRDwJGnX3Z"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "df['reviewText'] = df['reviewText'].apply(preprocess_text)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtrN3CJmqqxI"
      },
      "outputs": [],
      "source": [
        "# imprimir 5 filas con columna Positive igual a 0\n",
        "df[df['Positive'] == 0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK7SHupLnlvd"
      },
      "outputs": [],
      "source": [
        "# Perform sentiment analysis using NLTK Vader\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    sentiment = 1 if scores['pos'] > 0 else 0\n",
        "    return sentiment\n",
        "\n",
        "df['sentiment'] = df['reviewText'].apply(get_sentiment)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB_LBPCrq51w"
      },
      "outputs": [],
      "source": [
        "# Ejemplo del an√°lisis con 1 frase, para ver la salida del analizador\n",
        "\n",
        "texto = df['reviewText'][1]\n",
        "print(texto)\n",
        "analyzer.polarity_scores(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy_oXItDobku"
      },
      "outputs": [],
      "source": [
        "# imprimir 5 filas de df donde el sentimiento sea 0\n",
        "df[df['sentiment'] == 0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI-wLpxdoLLi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(df['Positive'], df['sentiment']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp-JdBVeoRHS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df['Positive'], df['sentiment']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5DIigZuZPEG"
      },
      "source": [
        "## ANEXO\n",
        "\n",
        "La librer√≠a SpaCy contiene un conjunto de frases ya disponibles para pruebas, accesibles a trav√©s de la estructura de datos 'sentences':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxZIFTT-g_cY"
      },
      "outputs": [],
      "source": [
        "# Frases de ejemplo disponibles en SpaCy:\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.es.examples import sentences\n",
        "\n",
        "# imprimir el contenido de sentences, una frase por l√≠nea\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "doc = nlp(sentences[0])\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    #print(token.text, token.pos_, token.dep_)\n",
        "    print(f\"Texto: {token.text}, Lema: {token.lemma_}, POS: {token.pos_}, Dependencia: {token.dep_}, Padre: {token.head.text}\")\n",
        "entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"\\nEntidades nombradas:\")\n",
        "print(entidades)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDTNBiiUZwyM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-5DIigZuZPEG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
