{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Sie0MOwNJi"
      },
      "source": [
        "## Pre-procesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-umnp5oz0Uq"
      },
      "source": [
        "**Normalización:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "57Stgf5ixyhu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT:  La NASA y la ONU lanzaron un cohete pa la Luna. La misión tuvo un costo de 3,5 millones de dólares. \n",
            "\n",
            "Conversión de siglas: La Administración Nacional de Aeronáutica y del Espacio y la Organización de las Naciones Unidas lanzaron un cohete pa la Luna. La misión tuvo un costo de 3,5 millones de dólares. \n",
            "\n",
            "Conversión a minúsculas: la administración nacional de aeronáutica y del espacio y la organización de las naciones unidas lanzaron un cohete pa la luna. la misión tuvo un costo de 3,5 millones de dólares. \n",
            "\n",
            "Expansión de contracciones: la administración nacional de aeronáutica y de el espacio y la organización de las naciones unidas lanzaron un cohete para la luna. la misión tuvo un costo de 3,5 millones de dólares. \n",
            "\n",
            "Normalización de números: la administración nacional de aeronáutica y de el espacio y la organización de las naciones unidas lanzaron un cohete para la luna. la misión tuvo un costo de <número>,<número> millones de dólares.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Diccionario para la conversión de siglas en español\n",
        "SIGLAS = {\n",
        "    \"ONU\": \"Organización de las Naciones Unidas\",\n",
        "    \"OMS\": \"Organización Mundial de la Salud\",\n",
        "    \"NASA\": \"Administración Nacional de Aeronáutica y del Espacio\",\n",
        "    \"UE\": \"Unión Europea\"\n",
        "}\n",
        "\n",
        "# Diccionario para la expansión de contracciones en español\n",
        "CONTRACTIONS = {\n",
        "    \"al\": \"a el\",\n",
        "    \"del\": \"de el\",\n",
        "    \"pa\": \"para\",\n",
        "    \"pal\": \"para el\"\n",
        "}\n",
        "\n",
        "\n",
        "# Función para convertir a minúsculas\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Función para normalizar números\n",
        "# Util para enfoques de análisis donde los números específicos no son relevantes o para evitar sesgos en el análisis de texto.\n",
        "def normalize_numbers(text):\n",
        "    # Convertir números a su forma escrita o eliminarlos, según sea necesario\n",
        "    normalized_text = re.sub(r'\\d+', '<número>', text)\n",
        "    return normalized_text\n",
        "\n",
        "# Función para expandir contracciones en español\n",
        "def expand_contractions(text):\n",
        "    contractions_pattern = re.compile(r'\\b(' + '|'.join(CONTRACTIONS.keys()) + r')\\b', re.IGNORECASE)\n",
        "    expanded_text = contractions_pattern.sub(lambda x: CONTRACTIONS[x.group().lower()], text)\n",
        "    return expanded_text\n",
        "\n",
        "# Función para convertir siglas a su forma completa\n",
        "def expand_acronyms(text):\n",
        "    acronyms_pattern = re.compile(r'\\b(' + '|'.join(SIGLAS.keys()) + r')\\b', re.IGNORECASE)\n",
        "    expanded_text = acronyms_pattern.sub(lambda x: SIGLAS[x.group().upper()], text)\n",
        "    return expanded_text\n",
        "\n",
        "# Ejemplo de uso\n",
        "sample_text = \"La NASA y la ONU lanzaron un cohete pa la Luna. La misión tuvo un costo de 3,5 millones de dólares.\"\n",
        "print(\"INPUT: \", sample_text,\"\\n\")\n",
        "\n",
        "# Conversión de siglas\n",
        "noacronyms_text = expand_acronyms(sample_text)\n",
        "print(\"Conversión de siglas:\", noacronyms_text, \"\\n\")\n",
        "\n",
        "# Conversión a minúsculas\n",
        "lowercase_text = to_lowercase(noacronyms_text)\n",
        "print(\"Conversión a minúsculas:\", lowercase_text, \"\\n\")\n",
        "\n",
        "# Expansión de contracciones\n",
        "expanded_text = expand_contractions(lowercase_text)\n",
        "print(\"Expansión de contracciones:\", expanded_text, \"\\n\")\n",
        "\n",
        "# Normalización de números\n",
        "normalized_text = normalize_numbers(expanded_text)\n",
        "print(\"Normalización de números:\", normalized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-chTFZTCz4ef"
      },
      "source": [
        "**Manejo de Emoticones:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsTZKO1kwU3B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ],
      "source": [
        "# %pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HPtW-eb5wP9r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT:  Voy al mercado pal almuerzo. Estoy tan feliz hoy! 😄 No puedo creerlo, aunque mi amigo está triste 😢. \n",
            "\n",
            "Traducción de emoticones: Voy al mercado pal almuerzo. Estoy tan feliz hoy! :grinning_face_with_smiling_eyes: No puedo creerlo, aunque mi amigo está triste :crying_face:. \n",
            "\n",
            "Eliminación de emoticones no relevantes: Voy al mercado pal almuerzo. Estoy tan feliz hoy!  No puedo creerlo, aunque mi amigo está triste .\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from emoji import demojize\n",
        "\n",
        "\n",
        "# Función para traducir emoticones a texto\n",
        "def translate_emoticons(text):\n",
        "    text = demojize(text)  # Convertir emoticones a sus descripciones textuales\n",
        "    emoticon_translation = {\n",
        "        \":sonriente:\": \"feliz\",\n",
        "        \":triste:\": \"triste\",\n",
        "        \":corazón:\": \"amor\",\n",
        "        \":pulgar_hacia_arriba:\": \"aprobación\",\n",
        "        \":pulgar_hacia_abajo:\": \"desaprobación\"\n",
        "    }\n",
        "    for emoticon, meaning in emoticon_translation.items():\n",
        "        text = text.replace(emoticon, meaning)\n",
        "    return text\n",
        "\n",
        "# Función para eliminar emoticones no relevantes\n",
        "def remove_irrelevant_emoticons(text):\n",
        "    text = demojize(text)\n",
        "    text = re.sub(r':[^:]+:', '', text)  # Eliminar todas las descripciones de emoticones\n",
        "    return text\n",
        "\n",
        "# Ejemplo de uso\n",
        "sample_text = \"Voy al mercado pal almuerzo. Estoy tan feliz hoy! 😄 No puedo creerlo, aunque mi amigo está triste 😢.\"\n",
        "print(\"INPUT: \", sample_text,\"\\n\")\n",
        "\n",
        "# Traducción de emoticones\n",
        "translated_text = translate_emoticons(sample_text)\n",
        "print(\"Traducción de emoticones:\", translated_text, \"\\n\")\n",
        "\n",
        "# Eliminación de emoticones no relevantes (si es necesario)\n",
        "clean_text = remove_irrelevant_emoticons(translated_text)\n",
        "print(\"Eliminación de emoticones no relevantes:\", clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhGhNjpx1hAm"
      },
      "source": [
        "**Limpieza de Texto:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulsZruxXN2IF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
            "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
          ]
        }
      ],
      "source": [
        "# %pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2c7UFBCey0s0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT:  \n",
            "Hola,, esttoy aprendiendo procesamiento de lenguaje natural.    Interesánte, eh!! \n",
            "\n",
            "Texto sin puntuación: \n",
            "Hola esttoy aprendiendo procesamiento de lenguaje natural    Interesánte eh \n",
            "\n",
            "Texto sin stop words: Hola esttoy aprendiendo procesamiento lenguaje natural Interesánte eh \n",
            "\n",
            "Texto limpio final: Hola esttoy aprendiendo procesamiento lenguaje natural Interesánte eh\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar las stop words en español\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Inicialización\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "# Función para eliminar puntuación\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Función para eliminar stop words\n",
        "def remove_stop_words(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Función para eliminar espacios en blanco adicionales\n",
        "def remove_extra_whitespace(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Ejemplo de uso\n",
        "sample_text = \"\\nHola,, esttoy aprendiendo procesamiento de lenguaje natural.    Interesánte, eh!!\"\n",
        "print(\"INPUT: \", sample_text,\"\\n\")\n",
        "\n",
        "# 1. Eliminación de Puntuación\n",
        "text_no_punct = remove_punctuation(sample_text)\n",
        "print(\"Texto sin puntuación:\", text_no_punct, \"\\n\")\n",
        "\n",
        "# 2. Eliminación de Stop Words\n",
        "text_no_stop_words = remove_stop_words(text_no_punct)\n",
        "print(\"Texto sin stop words:\", text_no_stop_words, \"\\n\")\n",
        "\n",
        "# 3. Eliminación de Espacios en Blanco Adicionales\n",
        "clean_text = remove_extra_whitespace(text_no_stop_words)\n",
        "print(\"Texto limpio final:\", clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa8w31jusQ0p"
      },
      "source": [
        "**ADICIONAL**: investigar librerías para corrección ortográfica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e_3Q0GxeeQj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n",
            "Texto con corrección ortográfica: Hola esto aprendiente procesamiento lenguaje natural Interesánte eh\n"
          ]
        }
      ],
      "source": [
        "# %pip install pyspellchecker\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker(language='es')\n",
        "\n",
        "def correct_spelling(text):\n",
        "  corrected_text = []\n",
        "  misspelled = spell.unknown(text.split())\n",
        "  for word in text.split():\n",
        "    if word in misspelled:\n",
        "      corrected_word = spell.correction(word)\n",
        "      corrected_text.append(corrected_word)\n",
        "    else:\n",
        "      corrected_text.append(word)\n",
        "  return \" \".join(corrected_text)\n",
        "\n",
        "corrected_text = correct_spelling(clean_text)\n",
        "print(\"Texto con corrección ortográfica:\", corrected_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z56FTcbzhcp"
      },
      "source": [
        "## Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTY57-JB5_8U"
      },
      "source": [
        "**Librería NLTK:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ev6NoevCNPPE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import NLTK and download required data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "74b7eJ74NvzJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Definición de NLTK:\n",
            "\n",
            "NTLK (Natural Language Toolkit) es una biblioteca de procesamiento de lenguaje natural en Python.\n",
            "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenización, análisis gramatical,\n",
            "etiquetado de partes del discurso, entre otros.\n",
            "\n",
            "\n",
            "Ejemplo de Tokenización:\n",
            "Oración original: Apple está buscando comprar una startup del Reino Unido por mil millones de dólares.\n",
            "Tokens: ['Apple', 'está', 'buscando', 'comprar', 'una', 'startup', 'del', 'Reino', 'Unido', 'por', 'mil', 'millones', 'de', 'dólares', '.']\n"
          ]
        }
      ],
      "source": [
        "# Define NLTK\n",
        "nltk_definition = \"\"\"\n",
        "NTLK (Natural Language Toolkit) es una biblioteca de procesamiento de lenguaje natural en Python.\n",
        "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenización, análisis gramatical,\n",
        "etiquetado de partes del discurso, entre otros.\n",
        "\"\"\"\n",
        "\n",
        "# Print definition\n",
        "print(\"\\nDefinición de NLTK:\")\n",
        "print(nltk_definition)\n",
        "\n",
        "# Tokenization example\n",
        "sentence = \"Apple está buscando comprar una startup del Reino Unido por mil millones de dólares.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nEjemplo de Tokenización:\")\n",
        "print(f\"Oración original: {sentence}\")\n",
        "print(f\"Tokens: {tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fDzQRIXDjILw"
      },
      "outputs": [],
      "source": [
        "cuento = \"En una noche tormentosa, Clara escuchó pasos en el ático, aunque vivía sola. \\\n",
        "  Subió las escaleras temblorosa, y al abrir la puerta, encontró una muñeca antigua mirándola fijamente. \\\n",
        "  De repente, la muñeca sonrió y susurró: 'Te estaba esperando.'\"\n",
        "\n",
        "# EJERCICIO: tokenizar palabras, frases, mostrar cantidad de tokens de ambas categorías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSXREfIVfi4"
      },
      "source": [
        "**Librería SpaCy:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18z9xliKNItL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
            "Collecting jinja2 (from spacy)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting setuptools (from spacy)\n",
            "  Downloading setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from spacy) (24.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.33.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-extensions>=4.12.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
            "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/31.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
            "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "Downloading numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
            "Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
            "Downloading pydantic_core-2.33.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Downloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (145 kB)\n",
            "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Downloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
            "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-extensions, spacy-loggers, spacy-legacy, shellingham, setuptools, numpy, murmurhash, mdurl, MarkupSafe, idna, cloudpathlib, charset-normalizer, certifi, catalogue, annotated-types, typing-inspection, srsly, smart-open, requests, pydantic-core, preshed, markdown-it-py, marisa-trie, jinja2, blis, rich, pydantic, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
            "Successfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 certifi-2025.1.31 charset-normalizer-3.4.1 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 idna-3.10 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 numpy-2.2.4 preshed-3.0.9 pydantic-2.11.1 pydantic-core-2.33.0 requests-2.32.3 rich-13.9.4 setuptools-78.1.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 typing-extensions-4.13.0 typing-inspection-0.4.0 urllib3-2.3.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
          ]
        }
      ],
      "source": [
        "# %pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNTlRtt5Vh1M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "# Descarga un modelo de lenguaje para el español en la biblioteca de PLN SpaCy,\n",
        "# optimizado para noticias (\"core_news\") y es de tamaño pequeño (\"sm\" para \"small\")\n",
        "# !python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HkE0_zUaVhse"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Definición de SpaCy:\n",
            "\n",
            "SpaCy es una biblioteca de procesamiento de lenguaje natural en Python, optimizada para rendimiento.\n",
            "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenización, etiquetado de entidades,\n",
            "análisis sintáctico, entre otros.\n",
            "\n",
            "Modelo cargado: core_news_sm\n",
            "\n",
            "Ejemplo de Tokenización con SpaCy:\n",
            "Oración original: Apple está buscando comprar una startup del Reino Unido por mil millones de dólares.\n",
            "Tokens: ['Apple', 'está', 'buscando', 'comprar', 'una', 'startup', 'del', 'Reino', 'Unido', 'por', 'mil', 'millones', 'de', 'dólares', '.']\n",
            "\n",
            "Ventajas de SpaCy:\n",
            "- Rápido: SpaCy está optimizado para un rendimiento rápido, lo que lo hace adecuado para aplicaciones en tiempo real.\n",
            "- Extensible: SpaCy permite la creación de pipelines personalizadas y la integración con otras bibliotecas.\n",
            "- Soporte para múltiples idiomas: SpaCy proporciona modelos para varios idiomas, facilitando el procesamiento de texto multilingüe.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import SpaCy\n",
        "import spacy\n",
        "\n",
        "# Define SpaCy\n",
        "spacy_definition = \"\"\"\n",
        "SpaCy es una biblioteca de procesamiento de lenguaje natural en Python, optimizada para rendimiento.\n",
        "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenización, etiquetado de entidades,\n",
        "análisis sintáctico, entre otros.\n",
        "\"\"\"\n",
        "\n",
        "# Print SpaCy definition\n",
        "print(\"\\nDefinición de SpaCy:\")\n",
        "print(spacy_definition)\n",
        "\n",
        "# model loading\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "print(\"Modelo cargado:\", nlp.meta[\"name\"])\n",
        "\n",
        "# Frase de prueba\n",
        "doc = nlp(\"Apple está buscando comprar una startup del Reino Unido por mil millones de dólares.\")\n",
        "\n",
        "# Print SpaCy results\n",
        "print(\"\\nEjemplo de Tokenización con SpaCy:\")\n",
        "print(f\"Oración original: {doc.text}\")\n",
        "print(\"Tokens:\", [token.text for token in doc])\n",
        "\n",
        "# Advantages of SpaCy\n",
        "spacy_advantages = \"\"\"\n",
        "Ventajas de SpaCy:\n",
        "- Rápido: SpaCy está optimizado para un rendimiento rápido, lo que lo hace adecuado para aplicaciones en tiempo real.\n",
        "- Extensible: SpaCy permite la creación de pipelines personalizadas y la integración con otras bibliotecas.\n",
        "- Soporte para múltiples idiomas: SpaCy proporciona modelos para varios idiomas, facilitando el procesamiento de texto multilingüe.\n",
        "\"\"\"\n",
        "print(spacy_advantages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ei-dtmvHjwv"
      },
      "source": [
        "## Lematización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntv42MrM8hn4"
      },
      "source": [
        "El lema es la forma base o raíz de una palabra. Por ejemplo, \"comprar\" es el lema de \"comprando\".\n",
        "\n",
        "La lematización permite normalizar diferentes formas de una palabra a su forma básica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qNnEXY419DVj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabra: corriendo, Lema: correr\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Cargar el modelo en español de spaCy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto = \"corriendo\"\n",
        "\n",
        "# Procesar el texto con spaCy\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Extraer y mostrar el lema\n",
        "for token in doc:\n",
        "    print(f\"Palabra: {token.text}, Lema: {token.lemma_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLASRVG4B8w0"
      },
      "source": [
        "## Radicalización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JLXDK33-F1XA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras original: ['corriendo', 'corredor', 'corrí', 'cocinero', 'cocina', 'cocinada', 'nacionalidad']\n",
            "Radicalización Porter  : ['corriendo', 'corredor', 'corrí', 'cocinero', 'cocina', 'cocinada', 'nacionalidad']\n",
            "Radicalización Snowball: ['corr', 'corredor', 'corr', 'cociner', 'cocin', 'cocin', 'nacional']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Ejemplos de palabras para radicalizar\n",
        "words = ['corriendo', 'corredor', 'corrí', 'cocinero', 'cocina', 'cocinada', 'nacionalidad']\n",
        "\n",
        "# Porter Stemmer\n",
        "stemmer = PorterStemmer() # only English!!!\n",
        "# Aplicar el Porter Stemmer\n",
        "stemmed_words_porter = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Instanciar el Snowball Stemmer para español\n",
        "stemmer = SnowballStemmer('spanish') # Snowball: extensión del Porter para multilenguaje\n",
        "# Aplicar el Snowball Stemmer\n",
        "stemmed_words_snowball = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Palabras original:\", words)\n",
        "print(\"Radicalización Porter  :\", stemmed_words_porter)\n",
        "print(\"Radicalización Snowball:\", stemmed_words_snowball)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gtRVf1HMjc5"
      },
      "source": [
        "¿Qué podemos analizar de estos resultados?\n",
        "\n",
        "Veamos ahora un ejemplo en inglés:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3fuC39BpHd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras original: ['running', 'runner', 'run', 'cooking', 'cookbook', 'cookery', 'generalization']\n",
            "Radicalización Porter  : ['run', 'runner', 'run', 'cook', 'cookbook', 'cookeri', 'gener']\n",
            "Radicalización Snowball: ['run', 'runner', 'run', 'cook', 'cookbook', 'cookeri', 'general']\n"
          ]
        }
      ],
      "source": [
        "# ENGLISH example\n",
        "\n",
        "# Ejemplos de palabras para radicalizar\n",
        "words = ['running', 'runner', 'run', 'cooking', 'cookbook', 'cookery', 'generalization']\n",
        "\n",
        "# Porter Stemmer\n",
        "stemmer = PorterStemmer() # only English!!!\n",
        "# Aplicar el Porter Stemmer\n",
        "stemmed_words_porter = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Instanciar el Snowball Stemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "# Aplicar el Snowball Stemmer\n",
        "stemmed_words_snowball = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Palabras original:\", words)\n",
        "print(\"Radicalización Porter  :\", stemmed_words_porter)\n",
        "print(\"Radicalización Snowball:\", stemmed_words_snowball)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwdR6uu9H02d"
      },
      "source": [
        "## Etiquetado de Partes del Discurso (POS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E9vtAw4B6ea"
      },
      "source": [
        "La etiqueta de parte del discurso (POS) indica la categoría gramatical de la palabra, algunas de las cuales son:\n",
        "- PROPN: Nombre propio.\n",
        "- AUX: Verbo auxiliar.\n",
        "- VERB: Verbo.\n",
        "- DET: Determinante.\n",
        "- NOUN: Sustantivo.\n",
        "- ADP: Preposición.\n",
        "- NUM: Número.\n",
        "- PUNCT: Signo de puntuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SCfgL5weG8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Etiquetado POS del SpaCy:\n",
            "\n",
            "El: DET\n",
            "gato: NOUN\n",
            "está: AUX\n",
            "debajo: ADV\n",
            "de: ADP\n",
            "la: DET\n",
            "mesa: NOUN\n",
            ".: PUNCT\n"
          ]
        }
      ],
      "source": [
        "# 1): usando spaCy\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Cargar el modelo en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Procesar el texto\n",
        "text = \"El gato está debajo de la mesa.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Mostrar las etiquetas de partes del discurso\n",
        "print(\"Etiquetado POS del SpaCy:\\n\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-p2zkE9AwG5P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oraciones tokenizadas:\n",
            "['El gato Felipe está en el tejado.', 'Parece que va a saltar.']\n",
            "\n",
            "Palabras tokenizadas:\n",
            "['El', 'gato', 'Felipe', 'está', 'en', 'el', 'tejado', '.', 'Parece', 'que', 'va', 'a', 'saltar', '.'] \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package cess_esp is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/gonzadzz/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/share/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(palabras,\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Etiquetar partes del discurso\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m tagged = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpalabras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspa\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Change 'tokens' to 'palabras' to use the correct variable\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEtiquetado POS del NLTK:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(tagged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m    275\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.weights = json.load(fin)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/gonzadzz/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/share/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# 2): usando NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import cess_esp\n",
        "\n",
        "# Descargar los datos necesarios\n",
        "nltk.download('cess_esp')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') # modelo pre-entrenado sólo para English!!!\n",
        "\n",
        "# TEXTO\n",
        "text = \"El gato Felipe está en el tejado. Parece que va a saltar.\"\n",
        "\n",
        "# Tokenización de oraciones\n",
        "oraciones = sent_tokenize(text, language='spanish')\n",
        "print(\"Oraciones tokenizadas:\")\n",
        "print(oraciones)\n",
        "\n",
        "# Tokenización de palabras\n",
        "palabras = word_tokenize(text, language='spanish') # The tokenized words are stored in the variable 'palabras'\n",
        "print(\"\\nPalabras tokenizadas:\")\n",
        "print(palabras,\"\\n\")\n",
        "\n",
        "# Etiquetar partes del discurso\n",
        "tagged = nltk.pos_tag(palabras, lang='spa') # Change 'tokens' to 'palabras' to use the correct variable\n",
        "print(\"Etiquetado POS del NLTK:\\n\")\n",
        "print(tagged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfth2RvGwCbB"
      },
      "source": [
        "Podemos ver que el español no está soportado todavía...\n",
        "\n",
        "Probemos otra aproximación: entrenar un etiquetador POS en español"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UFTwU0o3v_zh"
      },
      "outputs": [],
      "source": [
        "# Cargar el corpus de oraciones etiquetadas\n",
        "oraciones = cess_esp.tagged_sents()\n",
        "\n",
        "# Entrenar un etiquetador basado en unigramas y bigramas\n",
        "from nltk.tag import UnigramTagger, BigramTagger\n",
        "\n",
        "unigram_tagger = UnigramTagger(oraciones)\n",
        "bigram_tagger = BigramTagger(oraciones, backoff=unigram_tagger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fwe-iiFlkhPV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')], [('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explicó', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcción', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prevé', 'vmm02s0'), ('la', 'da0fs0'), ('utilización', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')], ...]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oraciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ESpXwIPSkp-E"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6030"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(oraciones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "X-TegOezxDT7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('El', 'da0ms0'), ('gato', 'ncms000'), ('Felipe', 'np0000p'), ('está', 'vmip3s0'), ('en', 'sps00'), ('el', 'da0ms0'), ('tejado', None), ('.', 'Fp'), ('Parece', 'vmip3s0'), ('que', 'cs'), ('va', 'vmip3s0'), ('a', 'sps00'), ('saltar', 'vmn0000'), ('.', 'Fp')]\n"
          ]
        }
      ],
      "source": [
        "# Prueba del etiquetador\n",
        "text = \"El gato Felipe está en el tejado. Parece que va a saltar.\"\n",
        "tokens = nltk.word_tokenize(text, language='spanish')\n",
        "etiquetas = bigram_tagger.tag(tokens)\n",
        "print(etiquetas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DEXrpXnxYYN"
      },
      "source": [
        "El corpus cess_esp contiene oraciones en español con sus respectivas etiquetas POS.\n",
        "\n",
        "Las etiquetas morfosintácticas están en el formato EAGLES (Expert Advisory Group on Language Engineering Standards), un estándar ampliamente usado en lingüística computacional para la anotación gramatical.\n",
        "\n",
        "Cada etiqueta es un código alfanumérico que describe la categoría gramatical de la palabra y algunas de sus propiedades morfosintácticas:\n",
        "\n",
        "Primera letra: Categoría gramatical principal.\n",
        "\n",
        "- n = Sustantivo (noun)\n",
        "- v = Verbo (verb)\n",
        "- a = Adjetivo (adjective)\n",
        "- d = Determinante (determiner)\n",
        "- s = Preposición o contracción (preposition)\n",
        "- c = Conjunción (conjunction)\n",
        "- p = Pronombre (pronoun)\n",
        "- f = Signo de puntuación (punctuation)\n",
        "\n",
        "Segunda letra: Tipo o subtipo de la categoría.\n",
        "\n",
        "- Para sustantivos (n), indica si es común (c) o propio (p).\n",
        "- Para verbos (v), se refiere al modo (indicativo, subjuntivo, etc.).\n",
        "\n",
        "Tercera letra y siguientes:\n",
        "- Características adicionales como género, número, tiempo verbal, etc.\n",
        "\n",
        "\n",
        "**Consideraciones**:\n",
        "\n",
        "El etiquetador entrenado con el corpus CESS-ESP puede no ser tan preciso para textos fuera de su dominio.\n",
        "\n",
        "Para un etiquetado POS más robusto en español, se recomiendan herramientas como spaCy o Stanza, que tienen modelos entrenados específicamente para español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LBxwYCorHDFP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Kubernetes', None), ('sirve', 'vmip3s0'), ('para', 'sps00'), ('automatizar', None), ('la', 'da0fs0'), ('orquestación', None), ('de', 'sps00'), ('contenedores', 'ncmp000'), ('.', 'Fp')]\n"
          ]
        }
      ],
      "source": [
        "# Frase fuera del contexto de entrenamiento\n",
        "text = \"Kubernetes sirve para automatizar la orquestación de contenedores.\"\n",
        "tokens = nltk.word_tokenize(text, language='spanish')\n",
        "etiquetas = bigram_tagger.tag(tokens)\n",
        "print(etiquetas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udydAe4qH8-_"
      },
      "source": [
        "Veamos un ejemplo en inglés:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "FYktpq3LIFcg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Kubernetes', 'NNS'), ('is', 'VBZ'), ('a', 'DT'), ('portable', 'JJ'), (',', ','), ('extensible', 'JJ'), (',', ','), ('open', 'JJ'), ('source', 'NN'), ('platform', 'NN'), ('for', 'IN'), ('managing', 'VBG'), ('containerized', 'JJ'), ('workloads', 'NNS'), ('and', 'CC'), ('services', 'NNS'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "text = \"Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services.\"\n",
        "tokens = word_tokenize(text, language='english')\n",
        "\n",
        "# Mostrar las etiquetas de partes del discurso usando Penn Treebank\n",
        "tagged = nltk.pos_tag(tokens, lang='eng')\n",
        "print(tagged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_lwtibFIiMA"
      },
      "source": [
        "Vemos que los códigos son diferentes. Aquí el NLTK usa el conjunto Penn Treebank, que es uno de los más comunes para etiquetado POS en inglés.\n",
        "\n",
        "Explicación de algunos códigos:\n",
        "- NNS: Sustantivo en plural (e.g., \"workloads\", \"services\").\n",
        "- VBZ: Verbo en tercera persona del singular en presente (e.g., \"is\").\n",
        "- DT: Determinante o artículo (e.g., \"a\").\n",
        "- JJ: Adjetivo (e.g., \"portable\", \"extensible\", \"containerized\", \"open\").\n",
        "- ,: Signo de puntuación (coma).\n",
        "- NN: Sustantivo en singular (e.g., \"source\", \"platform\").\n",
        "- IN: Preposición o conjunción subordinante (e.g., \"for\").\n",
        "- VBG: Verbo en gerundio o participio presente (e.g., \"managing\").\n",
        "- CC: Conjunción de coordinación (e.g., \"and\").\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAOrs-pEHEu8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textBlob\n",
            "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: nltk>=3.9 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from textBlob) (3.9.1)\n",
            "Requirement already satisfied: click in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (4.67.1)\n",
            "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: textBlob\n",
            "Successfully installed textBlob-0.19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/gonzadzz/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextBlob POS Tagging:\n",
            "El: NNP\n",
            "gato: NN\n",
            "está: NN\n",
            "debajo: NN\n",
            "de: IN\n",
            "la: FW\n",
            "mesa: FW\n"
          ]
        }
      ],
      "source": [
        "# 3): usando TextBlob\n",
        "# %pip install textBlob\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob.download_corpora import download_all\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "download_all()\n",
        "\n",
        "# Crear un objeto TextBlob\n",
        "text = \"El gato está debajo de la mesa.\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Etiquetar partes del discurso\n",
        "print(\"TextBlob POS Tagging:\")\n",
        "for word, tag in blob.tags:\n",
        "    print(f\"{word}: {tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1slgD-UqNyia"
      },
      "source": [
        "¿Qué tal el resultado...?\n",
        "\n",
        "Veamos ahora un ejemplo en inglés:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SULbMJQ91qkx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextBlob POS Tagging:\n",
            "The: DT\n",
            "cat: NN\n",
            "is: VBZ\n",
            "under: IN\n",
            "the: DT\n",
            "table: NN\n"
          ]
        }
      ],
      "source": [
        "# English example\n",
        "\n",
        "# Tokenizar el texto\n",
        "text = \"The cat is under the table.\"\n",
        "\n",
        "# Mostrar las etiquetas de partes del discurso\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Etiquetar partes del discurso\n",
        "print(\"TextBlob POS Tagging:\")\n",
        "for word, tag in blob.tags:\n",
        "    print(f\"{word}: {tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yEL-6zeHNkp"
      },
      "source": [
        "NOTAS:\n",
        "\n",
        "- NLTK: Ofrece flexibilidad y permite personalizar modelos, pero requiere configuraciones adicionales y puede no ser tan preciso en el español.\n",
        "\n",
        "- SpaCy: Proporciona modelos preentrenados precisos y fáciles de usar para múltiples idiomas.\n",
        "\n",
        "- TextBlob: Es una opción más sencilla y ligera, pero con soporte limitado para idiomas distintos del inglés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldjReL1-TsDW"
      },
      "source": [
        "#### Un poco más de información!\n",
        "\n",
        "*Dependencia (Dependency Parsing)*:\n",
        "\n",
        "La dependencia describe la relación sintáctica entre la palabra y su palabra \"padre\" (head). Las etiquetas de dependencia indican el papel gramatical de la palabra en la oración. Algunos ejemplos comunes son:\n",
        "- nsubj: Sujeto nominal.\n",
        "- aux: Auxiliar.\n",
        "- ROOT: Raíz de la oración.\n",
        "- xcomp: Complemento de objeto abierto.\n",
        "- det: Determinante.\n",
        "- obj: Objeto.\n",
        "- case: Preposición o palabra relacionada con la marca de caso.\n",
        "- nmod: Modificador nominal.\n",
        "- flat: Dependencia plana (usualmente para nombres propios compuestos).\n",
        "- nummod: Modificador numérico.\n",
        "- obl: Objeto oblicuo.\n",
        "- nmod: Modificador nominal.\n",
        "- punct: Puntuación.\n",
        "\n",
        "*Padre (Head)*:\n",
        "\n",
        "La palabra padre es el token principal al que está relacionado un token dado en la estructura sintáctica de la oración. En un árbol de dependencias, cada palabra está conectada a otra palabra (su padre) hasta llegar a la raíz de la oración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "s9Fr7js6UMml"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Análisis Sintáctico:\n",
            "Texto: El, Lema: el, POS: DET, Dependencia: det, Padre: gato\n",
            "Texto: gato, Lema: gato, POS: NOUN, Dependencia: nsubj, Padre: debajo\n",
            "Texto: está, Lema: estar, POS: AUX, Dependencia: cop, Padre: debajo\n",
            "Texto: debajo, Lema: debajo, POS: ADV, Dependencia: ROOT, Padre: debajo\n",
            "Texto: de, Lema: de, POS: ADP, Dependencia: case, Padre: mesa\n",
            "Texto: la, Lema: el, POS: DET, Dependencia: det, Padre: mesa\n",
            "Texto: mesa, Lema: mesa, POS: NOUN, Dependencia: obl, Padre: debajo\n",
            "Texto: ., Lema: ., POS: PUNCT, Dependencia: punct, Padre: debajo\n"
          ]
        }
      ],
      "source": [
        "# imprimir el análisis sintáctico, una entidad por línea\n",
        "print(\"\\nAnálisis Sintáctico:\")\n",
        "for token in doc:\n",
        "    print(f\"Texto: {token.text}, Lema: {token.lemma_}, POS: {token.pos_}, Dependencia: {token.dep_}, Padre: {token.head.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "kNzNO5mvxmIe"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m displacy\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Visualizar el análisis sintáctico\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdep\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfont\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAreal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdistance\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcompact\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjupyter\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/spacy/displacy/__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)"
          ]
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# Visualizar el análisis sintáctico\n",
        "displacy.render(doc,style = 'dep', options = {'font':'Areal','distance':100,'compact' : True,}, jupyter =True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou8yQK7HLVNx"
      },
      "source": [
        "## Reconocimiento de entidades nombradas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_71JPNlp1pYW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting es-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.8.0/es_core_news_lg-3.8.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-lg\n",
            "Successfully installed es-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n"
          ]
        }
      ],
      "source": [
        "# Modelo de lenguaje para el español en la biblioteca de PLN SpaCy,\n",
        "# optimizado para noticias (\"core_news\") y es de tamaño grande (\"lg\")\n",
        "!python -m spacy download es_core_news_lg #md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ1EP2J5Yp59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (24.2)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
            "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Installing collected packages: safetensors, pyyaml, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed filelock-3.18.0 fsspec-2025.3.0 huggingface-hub-0.29.3 pyyaml-6.0.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entidades Nombradas con SpaCy --español--:\n",
            "[('Raúl Alfonsín', 'PER'), ('Buenos Aires', 'LOC'), ('Argentina', 'LOC')]\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEntidades Nombradas con SpaCy --español--:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(entidades_spacy)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_spacy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m texto = \u001b[33m\"\u001b[39m\u001b[33mRaúl Alfonsín, a lawyer born in Buenos Aires, was the President of Argentina between 1983 and 1989.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m nlp_spacy = spacy.load(\u001b[33m\"\u001b[39m\u001b[33men_core_web_sm\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/spacy/displacy/__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)"
          ]
        }
      ],
      "source": [
        "%pip install transformers \n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from textblob import TextBlob\n",
        "from transformers import pipeline\n",
        "\n",
        "# Cargar el modelo de SpaCy para español\n",
        "nlp_spacy = spacy.load(\"es_core_news_lg\")\n",
        "\n",
        "# Texto a analizar\n",
        "texto = \"Raúl Alfonsín, abogado nacido en Buenos Aires, fue presidente de Argentina, entre 1983 y 1989.\"\n",
        "\n",
        "# 1. Reconocimiento de Entidades Nombradas con SpaCy\n",
        "doc_spacy = nlp_spacy(texto)\n",
        "entidades_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents]\n",
        "print(\"\\nEntidades Nombradas con SpaCy --español--:\")\n",
        "print(entidades_spacy)\n",
        "displacy.render(doc_spacy, style=\"ent\")\n",
        "\n",
        "\n",
        "texto = \"Raúl Alfonsín, a lawyer born in Buenos Aires, was the President of Argentina between 1983 and 1989.\"\n",
        "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "doc_spacy = nlp_spacy(texto)\n",
        "entidades_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents]\n",
        "print(\"\\nEntidades Nombradas con SpaCy --inglés--:\")\n",
        "print(entidades_spacy)\n",
        "displacy.render(doc_spacy, style=\"ent\")\n",
        "\n",
        "\n",
        "# 2. Reconocimiento de Entidades Nombradas con TextBlob\n",
        "# TextBlob no tiene un modelo preentrenado para NER en español, por lo que no es adecuado para este propósito\n",
        "# Para demostrar el uso de TextBlob en inglés, el siguiente código sería un ejemplo:\n",
        "blob = TextBlob(\"Raúl Alfonsín, a lawyer born in Buenos Aires, was the President of Argentina between 1983 and 1989.\")\n",
        "entidades_textblob = blob.noun_phrases\n",
        "print(\"\\nEntidades Nombradas con TextBlob (Inglés):\")\n",
        "print(entidades_textblob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVxufJNUSpX9"
      },
      "source": [
        "¿Qué diferencias se pueden apreciar en los resultados?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMXlOMknLwOO"
      },
      "source": [
        "## Ejercicio: Análisis de Texto en Español con NLTK y SpaCy\n",
        "\n",
        "Objetivos:\n",
        "- Aprender a tokenizar texto utilizando NLTK y SpaCy.\n",
        "- Comparar los resultados de tokenización entre ambas librerías.\n",
        "- Utilizar SpaCy para realizar el reconocimiento de entidades nombradas.\n",
        "- Realizar el análisis sintáctico de una oración con SpaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "H6da4WCpMDLF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparación de tokenizaciones:\n",
            "NTLK : ['El', 'presidente', 'Barack', 'Obama', 'visitó', 'París', 'en', 'julio', 'de', '2023', 'para', 'una', 'cumbre', 'internacional', '.']\n",
            "SpaCy: ['El', 'presidente', 'Barack', 'Obama', 'visitó', 'París', 'en', 'julio', 'de', '2023', 'para', 'una', 'cumbre', 'internacional', '.']\n",
            "\n",
            "Entidades Nombradas con SpaCy:\n",
            "[('Barack Obama visitó París', 'PER')]\n",
            "\n",
            "Análisis sintáctico completo con SpaCy:\n",
            "Texto: El, Lema: el, POS: DET, Dependencia: det, Padre: presidente\n",
            "Texto: presidente, Lema: presidente, POS: NOUN, Dependencia: nsubj, Padre: visitó\n",
            "Texto: Barack, Lema: Barack, POS: PROPN, Dependencia: appos, Padre: presidente\n",
            "Texto: Obama, Lema: Obama, POS: PROPN, Dependencia: flat, Padre: Barack\n",
            "Texto: visitó, Lema: visitar, POS: VERB, Dependencia: ROOT, Padre: visitó\n",
            "Texto: París, Lema: París, POS: PROPN, Dependencia: obj, Padre: visitó\n",
            "Texto: en, Lema: en, POS: ADP, Dependencia: case, Padre: julio\n",
            "Texto: julio, Lema: julio, POS: NOUN, Dependencia: obl, Padre: visitó\n",
            "Texto: de, Lema: de, POS: ADP, Dependencia: case, Padre: 2023\n",
            "Texto: 2023, Lema: 2023, POS: NUM, Dependencia: compound, Padre: julio\n",
            "Texto: para, Lema: para, POS: ADP, Dependencia: case, Padre: cumbre\n",
            "Texto: una, Lema: uno, POS: DET, Dependencia: det, Padre: cumbre\n",
            "Texto: cumbre, Lema: cumbre, POS: NOUN, Dependencia: obl, Padre: visitó\n",
            "Texto: internacional, Lema: internacional, POS: ADJ, Dependencia: amod, Padre: cumbre\n",
            "Texto: ., Lema: ., POS: PUNCT, Dependencia: punct, Padre: visitó\n"
          ]
        }
      ],
      "source": [
        "# Texto a analizar\n",
        "texto = \"El presidente Barack Obama visitó París en julio de 2023 para una cumbre internacional.\"\n",
        "#texto = \"El Dr. Juan Pérez, un reconocido neurocirujano de Nueva York, intervino exitosamente a la Sra. Julia González.\"\n",
        "\n",
        "# Tokenización con NLTK\n",
        "tokens_nltk = nltk.word_tokenize(texto)\n",
        "\n",
        "# Tokenización con SpaCy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "doc = nlp(texto)\n",
        "tokens_spacy = [token.text for token in doc]\n",
        "\n",
        "# Comparación de Tokenización\n",
        "print(\"\\nComparación de tokenizaciones:\")\n",
        "print(\"NTLK :\", tokens_nltk)\n",
        "print(\"SpaCy:\", tokens_spacy)\n",
        "\n",
        "\n",
        "# Reconocimiento de Entidades Nombradas con SpaCy\n",
        "entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"\\nEntidades Nombradas con SpaCy:\")\n",
        "print(entidades)\n",
        "\n",
        "# Análisis Sintáctico con SpaCy\n",
        "print(\"\\nAnálisis sintáctico completo con SpaCy:\")\n",
        "for token in doc:\n",
        "    print(f\"Texto: {token.text}, Lema: {token.lemma_}, POS: {token.pos_}, Dependencia: {token.dep_}, Padre: {token.head.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTp-M8gzMGGP"
      },
      "source": [
        "Discusión para los resultados con ambras frases:\n",
        "\n",
        "- ¿Qué diferencias notaste en la tokenización entre NLTK y SpaCy?\n",
        "- ¿Qué ventajas ofrece SpaCy en términos de rendimiento y capacidades adicionales?\n",
        "\n",
        "Discusión general:\n",
        "\n",
        "- ¿Cómo puede el reconocimiento de entidades nombradas ayudar en el análisis de texto?\n",
        "- ¿Qué información útil puedes obtener del análisis sintáctico realizado con SpaCy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcdiSK1WSziR"
      },
      "source": [
        "1) La tokenización de SpaCy es más precisa y maneja mejor los signos de puntuación y las entidades compuestas en comparación con NLTK.\n",
        "\n",
        "2) SpaCy ofrece un rendimiento superior y capacidades adicionales como el reconocimiento de entidades nombradas y el análisis sintáctico, haciéndolo más adecuado para aplicaciones en tiempo real.\n",
        "\n",
        "3) El reconocimiento de entidades nombradas facilita la identificación de elementos clave en el texto, como nombres de personas, lugares y organizaciones, mejorando la comprensión y el análisis contextual.\n",
        "\n",
        "4) El análisis sintáctico con SpaCy proporciona información detallada sobre las relaciones gramaticales entre las palabras, lo que ayuda a entender la estructura y el significado subyacente de las oraciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuQh6_3OmiZC"
      },
      "source": [
        "#### Ejemplo de análisis de sentimiento usando NLTK\n",
        "\n",
        "Fuente:\n",
        "https://www.datacamp.com/es/tutorial/text-analytics-beginners-nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "XU-Kt3ZImsNu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from pandas) (2.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from scikit-learn) (2.2.4)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
            "Successfully installed scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "reviewText",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Positive",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "ec963234-d84a-4100-b8ff-eb3bb657d9b3",
              "rows": [
                [
                  "0",
                  "This is a one of the best apps acording to a bunch of people and I agree it has bombs eggs pigs TNT king pigs and realustic stuff",
                  "1"
                ],
                [
                  "1",
                  "This is a pretty good version of the game for being free. There are LOTS of different levels to play. My kids enjoy it a lot too.",
                  "1"
                ],
                [
                  "2",
                  "this is a really cool game. there are a bunch of levels and you can find golden eggs. super fun.",
                  "1"
                ],
                [
                  "3",
                  "This is a silly game and can be frustrating, but lots of fun and definitely recommend just as a fun time.",
                  "1"
                ],
                [
                  "4",
                  "This is a terrific game on any pad. Hrs of fun.  My grandkids love it. Great entertainment when waiting in long lines",
                  "1"
                ],
                [
                  "5",
                  "This is a very entertaining game!  You don't have to be smart to play it.  I guess that's why I like it...it's easy and fun and that's what games are suppose to be.  Be warned: this game is highly addictive.",
                  "1"
                ],
                [
                  "6",
                  "this is awesome and you don't need wi ti to play trust me. it is really fun and addicting. there are like 100 levels it is even free don't waste your money on the expensive one I mean seriously. get the app",
                  "1"
                ],
                [
                  "7",
                  "this is awesome I bet no one even reads the reviews because they know this game is so good that they don't need to",
                  "1"
                ],
                [
                  "8",
                  "This is basicly the free version but with ads. That's actually awesome!!!! It's addicting and free at the same time really. I'd reccomend it.",
                  "1"
                ],
                [
                  "9",
                  "this is by far the best free app that is available anywhere. it has helped pass the time when nothing else would do. don't pass this one up. PS I hate this 20 word minimum!",
                  "1"
                ],
                [
                  "10",
                  "This is definitely a great game.  I have to get my 6-year-old grand-nephew to teach me the tricks.  I have figured out some of them, but some configurations are tough to beat.  I don't particularly care spending about 45 minutes  completing a level, but ",
                  "1"
                ],
                [
                  "11",
                  "this. is fun an time consuming. works great on my kindle fire I really like this game so does my btother",
                  "1"
                ],
                [
                  "12",
                  "This is good if you like physics games, free games, or bird games. Not like free version on ipod. You get all the levels and the only adds pop up in the corner and are barely noticeable.",
                  "1"
                ],
                [
                  "13",
                  "This is great! This is my tried & true game to cure boredom! Its great to try & get 3 stars on every level!",
                  "1"
                ],
                [
                  "14",
                  "this is one of the best apps in the world along with all the other angry birds games. great for passing time by.",
                  "1"
                ],
                [
                  "15",
                  "This is super fun though a little frustrating at times!!!!!!!!! I loved it before I got my kindle I would beg my brother and dad to play. But when I get it I play it all the time!!!!!!",
                  "1"
                ],
                [
                  "16",
                  "this is the ad free version of angry birds. this is the old fashioned version of the angry birds series. it is s huge and partially difficult game. if you have played angry birds before or this is the first time, you'll love it",
                  "1"
                ],
                [
                  "17",
                  "This is the best app ever. It makes it way easier to babysitt a mental kid. this app is very fun.",
                  "1"
                ],
                [
                  "18",
                  "this is the best app I ever downloaded for most of you people who say 1 2 3 or 4 stars you don't know much about what your supposed to do I purfor 5 stars for this app",
                  "1"
                ],
                [
                  "19",
                  "THIS IS THE BESTEST GAME EVER OMG. IM SO EXCITED :TO PLAY OMG!!!!!!! OMG OMG OMG I LOVE ANGRY BIRDS SO MUCH ITS DA BOMB DIGGETY SO COOL OMG OMG OMG OMG OMG",
                  "1"
                ],
                [
                  "20",
                  "This is the best game ever!!!!!  it is so fun and addicting. there is so many different episodes that are amazing. love it!",
                  "1"
                ],
                [
                  "21",
                  "this is the BEST game ever you should buy it the most awesome game in the whole wide world.  :D",
                  "1"
                ],
                [
                  "22",
                  "this is the best game they could think of.I downloaded it and my fav. character is the original pig.yay love it",
                  "1"
                ],
                [
                  "23",
                  "this is the first angry birds. so when you get in2 this one it'll leas to you wanting angry birds seasons and angry birds Rio. But they're all fantastic!!!!!!!!",
                  "1"
                ],
                [
                  "24",
                  "This is the first app that I installed onKindle Fire, Full Color 7\" Multi-touch Display, Wi-Fiwhich I purchased recently from Amazon. Angry Birds (free version) works smoothly on Kindle fire.Love the smooth graphics! It responds perfectly and It is a ple",
                  "1"
                ],
                [
                  "25",
                  "this is the full version except it has ads. It is one of the best free apps on the app store. For IOS, the free version is like a demo. This is a better buy. You save 99 cents on the exact same thing as the paid version. I recommend it to everyone. A mus",
                  "1"
                ],
                [
                  "26",
                  "this is the game to get. those other bad comments r stupid. this app doesn't need wifi.  I like this app a lot its free and who doesn't love angry birds!!!",
                  "1"
                ],
                [
                  "27",
                  "this is the go to game for me... love it.  I find this much better than most other games out there",
                  "1"
                ],
                [
                  "28",
                  "this is the one of the best apps becase it is a good time consuming games were ever you go becase you do not wifi",
                  "1"
                ],
                [
                  "29",
                  "This is the original anfry birds game but it only works on the original Kindle Fire and not on the new HD version.NEW NOTE - This has now been updated to work on the new Kindle Fire HD. Of course that did not happen until I bought the paid version, but n",
                  "1"
                ],
                [
                  "30",
                  "This is very popular game I've been hearing for a long time till my friends dared me to download it and play it.  Be damned that I got addicted to this game.  I love it so much.  I just turned my husband into this game, too.  Soon, I'll get my son on it,",
                  "1"
                ],
                [
                  "31",
                  "This keeps me busy all the time. I am really happy about this app, I have never ever had any problems.P.S.Good luck with those green little boogers!",
                  "1"
                ],
                [
                  "32",
                  "This physics based game is perfect for people of all ages to spend extra time on a mobile device. Yet it still offers some challenge. It works very well on the kindle fire.",
                  "1"
                ],
                [
                  "33",
                  "This seems to be popular when my kindle fire is out of my hands.  Lots of fun and free time for me plus the graphics are very, very nice.",
                  "1"
                ],
                [
                  "34",
                  "This so much fun! My daughter has vthis app for her IPhone but I never realized how much fun it is until I downloaded for my Kindle Fire. I highly recommend getting this app...works great on the Kindle Fire!",
                  "1"
                ],
                [
                  "35",
                  "This was a very entertaining and challenging game with several levels to seek improvement. The ads did not overrun it and, for free, this was an excellent program.",
                  "1"
                ],
                [
                  "36",
                  "This was my first time to play Angry Birds and I don'tthink I could have had a better first impression!  Because of this, I will purchase every Angry Birds gameI can get my hands on!  Now I see what all the 'fuss' is about!!!",
                  "1"
                ],
                [
                  "37",
                  "this was one of the first apps I got on my kindle fire and I love playing it. it is great for passing the time. the ads don't bother me at all they are in a small little box at the top right of the screen and I hardly notice them. it is not worth paying ",
                  "1"
                ],
                [
                  "38",
                  "This was the 1st app I ever downloaded on my Kindle Fire and I LOVE it! And you do not need wi-fi to play it!  I think it's a great app and time-waster.",
                  "1"
                ],
                [
                  "39",
                  "this was the best game of all time to come to a phone. I would put it up there with snake on the old nokia bricks.",
                  "1"
                ],
                [
                  "40",
                  "Very addictive. I can play this for hours. The first game I installed to my fire, and my most played.",
                  "1"
                ],
                [
                  "41",
                  "Very fun and challenges you, I always try and get three stars, nice graphics and gameplay, always a good way to kill time.",
                  "1"
                ],
                [
                  "42",
                  "Very fun and very addictingSo good u should pay money for itOnly winners play this game so get it!!!!!!!!!!!!!!!!!!!!!!!",
                  "1"
                ],
                [
                  "43",
                  "very fun game and all the other people who say it freezes and don't work you must have to many games on you're. device",
                  "1"
                ],
                [
                  "44",
                  "very fun very fun very fun very fun, keeps me busy. I cannot stop playing. very challenging,  this game can make you angry,sometimes , but then I win.",
                  "1"
                ],
                [
                  "45",
                  "Very high quality for a game. Fast, and crystal clear. Amazing that a game so popular, would be FREE! All levels to accomplish to become an Angry Birds Pro.~ Jessie",
                  "1"
                ],
                [
                  "46",
                  "very very very very very very very very awesome &iexcl;!!!!! !!'! !!!!!!!! !!!!! !!!!! x. !&iexcl;!! cgdixjcv dnjdxjx gcfjxcb fvcv hxgrdh",
                  "1"
                ],
                [
                  "47",
                  "Watch out!  This is a very addicting game, but it is very fun for all ages!  Good way to kill time, or waste time.",
                  "1"
                ],
                [
                  "48",
                  "We all love this game.  It is fun for all ages.  We all cant wait till someone gets tired of it so we can get the tablet and play it.",
                  "1"
                ],
                [
                  "49",
                  "We downloaded the free version for our tween's new Kindle Fire and everyone (from age 9 to 74) has had a blast playing the game which has many levels and challenges.  The graphics and sounds are great too.",
                  "1"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 20000
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>Positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This is a one of the best apps acording to a b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is a pretty good version of the game for ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this is a really cool game. there are a bunch ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is a silly game and can be frustrating, b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This is a terrific game on any pad. Hrs of fun...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>this app is fricken stupid.it froze on the kin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>Please add me!!!!! I need neighbors! Ginger101...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>love it!  this game. is awesome. wish it had m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>I love love love this app on my side of fashio...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>This game is a rip off. Here is a list of thin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              reviewText  Positive\n",
              "0      This is a one of the best apps acording to a b...         1\n",
              "1      This is a pretty good version of the game for ...         1\n",
              "2      this is a really cool game. there are a bunch ...         1\n",
              "3      This is a silly game and can be frustrating, b...         1\n",
              "4      This is a terrific game on any pad. Hrs of fun...         1\n",
              "...                                                  ...       ...\n",
              "19995  this app is fricken stupid.it froze on the kin...         0\n",
              "19996  Please add me!!!!! I need neighbors! Ginger101...         1\n",
              "19997  love it!  this game. is awesome. wish it had m...         1\n",
              "19998  I love love love this app on my side of fashio...         1\n",
              "19999  This game is a rip off. Here is a list of thin...         0\n",
              "\n",
              "[20000 rows x 2 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Start coding import pandas as pd \n",
        "%pip install pandas\n",
        "%pip install scikit-learn\n",
        "\n",
        " \n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# download nltk corpus\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "# Load the amazon review dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/amazon.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "s83nexRNpUHU"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "reviewText",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Positive",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "4ab17753-9728-4b3e-96fe-a4eb1799994a",
              "rows": [
                [
                  "78",
                  "They call it three next mario??? Are the 'professional reviewers' high??? Ask you do is fling birds at bones again and again and again. Has zero replay value.",
                  "0"
                ],
                [
                  "79",
                  "Angry People in my book!  Any moron who would waste their time playing azimuth slingshot and call it a cultural phenomenon is ... what is the word (???) ... oh yes, a moron!!!  Those 5 minutes I played this thing still scar me!",
                  "0"
                ],
                [
                  "80",
                  "Argry Birds is for free to play a games on your kindlefireBecause it for free. and to play right.",
                  "0"
                ],
                [
                  "81",
                  "cant stand this game. love the concept and played many of the same type. I have to give the developer credit for creating a ton of levels, keeps you going for hours. sounds are irritating. the constant chirps are annoying as heck, play this with the soun",
                  "0"
                ],
                [
                  "82",
                  "Don't really know the point of this game.  It's almost impossible to knock the stupid birds down.  Why waste your time on this one?",
                  "0"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>Positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>They call it three next mario??? Are the 'prof...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>Angry People in my book!  Any moron who would ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>Argry Birds is for free to play a games on you...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>cant stand this game. love the concept and pla...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>Don't really know the point of this game.  It'...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           reviewText  Positive\n",
              "78  They call it three next mario??? Are the 'prof...         0\n",
              "79  Angry People in my book!  Any moron who would ...         0\n",
              "80  Argry Birds is for free to play a games on you...         0\n",
              "81  cant stand this game. love the concept and pla...         0\n",
              "82  Don't really know the point of this game.  It'...         0"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# imprimir 5 filas con columna Positive igual a 0\n",
        "df[df['Positive'] == 0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "FAFRDwJGnX3Z"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "reviewText",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Positive",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "16f38858-afee-4ff7-8692-d39326757241",
              "rows": [
                [
                  "0",
                  "one best apps acording bunch people agree bomb egg pig tnt king pig realustic stuff",
                  "1"
                ],
                [
                  "1",
                  "pretty good version game free . lot different level play . kid enjoy lot .",
                  "1"
                ],
                [
                  "2",
                  "really cool game . bunch level find golden egg . super fun .",
                  "1"
                ],
                [
                  "3",
                  "silly game frustrating , lot fun definitely recommend fun time .",
                  "1"
                ],
                [
                  "4",
                  "terrific game pad . hr fun . grandkids love . great entertainment waiting long line",
                  "1"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>Positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one best apps acording bunch people agree bomb...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pretty good version game free . lot different ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>really cool game . bunch level find golden egg...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>silly game frustrating , lot fun definitely re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>terrific game pad . hr fun . grandkids love . ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          reviewText  Positive\n",
              "0  one best apps acording bunch people agree bomb...         1\n",
              "1  pretty good version game free . lot different ...         1\n",
              "2  really cool game . bunch level find golden egg...         1\n",
              "3  silly game frustrating , lot fun definitely re...         1\n",
              "4  terrific game pad . hr fun . grandkids love . ...         1"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "df['reviewText'] = df['reviewText'].apply(preprocess_text)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xtrN3CJmqqxI"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "reviewText",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Positive",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "7e1c7f84-bea4-4efc-af00-3f90fdb37143",
              "rows": [
                [
                  "78",
                  "call three next mario ? ? ? 'professional reviewer ' high ? ? ? ask fling bird bone . zero replay value .",
                  "0"
                ],
                [
                  "79",
                  "angry people book ! moron would waste time playing azimuth slingshot call cultural phenomenon ... word ( ? ? ? ) ... oh yes , moron ! ! ! 5 minute played thing still scar !",
                  "0"
                ],
                [
                  "80",
                  "argry bird free play game kindlefirebecause free . play right .",
                  "0"
                ],
                [
                  "81",
                  "cant stand game . love concept played many type . give developer credit creating ton level , keep going hour . sound irritating . constant chirp annoying heck , play soun",
                  "0"
                ],
                [
                  "82",
                  "n't really know point game . 's almost impossible knock stupid bird . waste time one ?",
                  "0"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>Positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>call three next mario ? ? ? 'professional revi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>angry people book ! moron would waste time pla...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>argry bird free play game kindlefirebecause fr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>cant stand game . love concept played many typ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>n't really know point game . 's almost impossi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           reviewText  Positive\n",
              "78  call three next mario ? ? ? 'professional revi...         0\n",
              "79  angry people book ! moron would waste time pla...         0\n",
              "80  argry bird free play game kindlefirebecause fr...         0\n",
              "81  cant stand game . love concept played many typ...         0\n",
              "82  n't really know point game . 's almost impossi...         0"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# imprimir 5 filas con columna Positive igual a 0\n",
        "df[df['Positive'] == 0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tK7SHupLnlvd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "reviewText",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Positive",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "sentiment",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "0771787a-b404-4ee0-80c8-388bd11760b8",
              "rows": [
                [
                  "0",
                  "one best apps acording bunch people agree bomb egg pig tnt king pig realustic stuff",
                  "1",
                  "1"
                ],
                [
                  "1",
                  "pretty good version game free . lot different level play . kid enjoy lot .",
                  "1",
                  "1"
                ],
                [
                  "2",
                  "really cool game . bunch level find golden egg . super fun .",
                  "1",
                  "1"
                ],
                [
                  "3",
                  "silly game frustrating , lot fun definitely recommend fun time .",
                  "1",
                  "1"
                ],
                [
                  "4",
                  "terrific game pad . hr fun . grandkids love . great entertainment waiting long line",
                  "1",
                  "1"
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>Positive</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one best apps acording bunch people agree bomb...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pretty good version game free . lot different ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>really cool game . bunch level find golden egg...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>silly game frustrating , lot fun definitely re...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>terrific game pad . hr fun . grandkids love . ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          reviewText  Positive  sentiment\n",
              "0  one best apps acording bunch people agree bomb...         1          1\n",
              "1  pretty good version game free . lot different ...         1          1\n",
              "2  really cool game . bunch level find golden egg...         1          1\n",
              "3  silly game frustrating , lot fun definitely re...         1          1\n",
              "4  terrific game pad . hr fun . grandkids love . ...         1          1"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Perform sentiment analysis using NLTK Vader\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    sentiment = 1 if scores['pos'] > 0 else 0\n",
        "    return sentiment\n",
        "\n",
        "df['sentiment'] = df['reviewText'].apply(get_sentiment)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "GB_LBPCrq51w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pretty good version game free . lot different level play . kid enjoy lot .\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.9325}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ejemplo del análisis con 1 frase, para ver la salida del analizador\n",
        "\n",
        "texto = df['reviewText'][1]\n",
        "print(texto)\n",
        "analyzer.polarity_scores(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Dy_oXItDobku"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "reviewText",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Positive",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "sentiment",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "cf376423-80eb-4d0e-86c5-15a89e73a13c",
              "rows": [
                [
                  "53",
                  "angry bird space folding kindle fire ? jrdhdhdh fhd dud dvd dud db 's dhdjsbd fhdbfjs sjdddhdx dud dud",
                  "1",
                  "0"
                ],
                [
                  "58",
                  "wait see say doctor , dmv , wife get done shopping",
                  "1",
                  "0"
                ],
                [
                  "82",
                  "n't really know point game . 's almost impossible knock stupid bird . waste time one ?",
                  "0",
                  "0"
                ],
                [
                  "90",
                  "n't like .i think kid . really many game",
                  "0",
                  "0"
                ],
                [
                  "91",
                  "got end pre-installed version . confused , checked online see app missing entire level . one post suggested downloading newer version . crash moment start .",
                  "0",
                  "0"
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>Positive</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>angry bird space folding kindle fire ? jrdhdhd...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>wait see say doctor , dmv , wife get done shop...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>n't really know point game . 's almost impossi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>n't like .i think kid . really many game</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>got end pre-installed version . confused , che...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           reviewText  Positive  sentiment\n",
              "53  angry bird space folding kindle fire ? jrdhdhd...         1          0\n",
              "58  wait see say doctor , dmv , wife get done shop...         1          0\n",
              "82  n't really know point game . 's almost impossi...         0          0\n",
              "90           n't like .i think kid . really many game         0          0\n",
              "91  got end pre-installed version . confused , che...         0          0"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# imprimir 5 filas de df donde el sentimiento sea 0\n",
        "df[df['sentiment'] == 0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "jI-wLpxdoLLi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1131  3636]\n",
            " [  576 14657]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(df['Positive'], df['sentiment']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "xp-JdBVeoRHS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.24      0.35      4767\n",
            "           1       0.80      0.96      0.87     15233\n",
            "\n",
            "    accuracy                           0.79     20000\n",
            "   macro avg       0.73      0.60      0.61     20000\n",
            "weighted avg       0.77      0.79      0.75     20000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df['Positive'], df['sentiment']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5DIigZuZPEG"
      },
      "source": [
        "## ANEXO\n",
        "\n",
        "La librería SpaCy contiene un conjunto de frases ya disponibles para pruebas, accesibles a través de la estructura de datos 'sentences':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kxZIFTT-g_cY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apple está buscando comprar una startup del Reino Unido por mil millones de dólares.\n",
            "Texto: Apple, Lema: Apple, POS: PROPN, Dependencia: nsubj, Padre: buscando\n",
            "Texto: está, Lema: estar, POS: AUX, Dependencia: aux, Padre: buscando\n",
            "Texto: buscando, Lema: buscar, POS: VERB, Dependencia: ROOT, Padre: buscando\n",
            "Texto: comprar, Lema: comprar, POS: VERB, Dependencia: xcomp, Padre: buscando\n",
            "Texto: una, Lema: uno, POS: DET, Dependencia: det, Padre: startup\n",
            "Texto: startup, Lema: startup, POS: NOUN, Dependencia: obj, Padre: comprar\n",
            "Texto: del, Lema: del, POS: ADP, Dependencia: case, Padre: Reino\n",
            "Texto: Reino, Lema: Reino, POS: PROPN, Dependencia: nmod, Padre: startup\n",
            "Texto: Unido, Lema: Unido, POS: PROPN, Dependencia: flat, Padre: Reino\n",
            "Texto: por, Lema: por, POS: ADP, Dependencia: case, Padre: millones\n",
            "Texto: mil, Lema: mil, POS: NUM, Dependencia: nummod, Padre: millones\n",
            "Texto: millones, Lema: millón, POS: NOUN, Dependencia: obl, Padre: comprar\n",
            "Texto: de, Lema: de, POS: ADP, Dependencia: case, Padre: dólares\n",
            "Texto: dólares, Lema: dólares, POS: NOUN, Dependencia: nmod, Padre: millones\n",
            "Texto: ., Lema: ., POS: PUNCT, Dependencia: punct, Padre: buscando\n",
            "\n",
            "Entidades nombradas:\n",
            "[('Apple', 'ORG'), ('Reino Unido', 'LOC')]\n"
          ]
        }
      ],
      "source": [
        "# Frases de ejemplo disponibles en SpaCy:\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.es.examples import sentences\n",
        "\n",
        "# imprimir el contenido de sentences, una frase por línea\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "doc = nlp(sentences[0])\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    #print(token.text, token.pos_, token.dep_)\n",
        "    print(f\"Texto: {token.text}, Lema: {token.lemma_}, POS: {token.pos_}, Dependencia: {token.dep_}, Padre: {token.head.text}\")\n",
        "entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"\\nEntidades nombradas:\")\n",
        "print(entidades)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-5DIigZuZPEG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
