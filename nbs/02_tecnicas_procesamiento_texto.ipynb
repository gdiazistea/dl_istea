{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Sie0MOwNJi"
      },
      "source": [
        "## Pre-procesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-umnp5oz0Uq"
      },
      "source": [
        "**Normalización:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "57Stgf5ixyhu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT:  La NASA y la ONU lanzaron un cohete pa la Luna. La misión tuvo un costo de 3,5 millones de dólares. \n",
            "\n",
            "Conversión de siglas: La Administración Nacional de Aeronáutica y del Espacio y la Organización de las Naciones Unidas lanzaron un cohete pa la Luna. La misión tuvo un costo de 3,5 millones de dólares. \n",
            "\n",
            "Conversión a minúsculas: la administración nacional de aeronáutica y del espacio y la organización de las naciones unidas lanzaron un cohete pa la luna. la misión tuvo un costo de 3,5 millones de dólares. \n",
            "\n",
            "Expansión de contracciones: la administración nacional de aeronáutica y de el espacio y la organización de las naciones unidas lanzaron un cohete para la luna. la misión tuvo un costo de 3,5 millones de dólares. \n",
            "\n",
            "Normalización de números: la administración nacional de aeronáutica y de el espacio y la organización de las naciones unidas lanzaron un cohete para la luna. la misión tuvo un costo de <número>,<número> millones de dólares.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Diccionario para la conversión de siglas en español\n",
        "SIGLAS = {\n",
        "    \"ONU\": \"Organización de las Naciones Unidas\",\n",
        "    \"OMS\": \"Organización Mundial de la Salud\",\n",
        "    \"NASA\": \"Administración Nacional de Aeronáutica y del Espacio\",\n",
        "    \"UE\": \"Unión Europea\"\n",
        "}\n",
        "\n",
        "# Diccionario para la expansión de contracciones en español\n",
        "CONTRACTIONS = {\n",
        "    \"al\": \"a el\",\n",
        "    \"del\": \"de el\",\n",
        "    \"pa\": \"para\",\n",
        "    \"pal\": \"para el\"\n",
        "}\n",
        "\n",
        "\n",
        "# Función para convertir a minúsculas\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Función para normalizar números\n",
        "# Util para enfoques de análisis donde los números específicos no son relevantes o para evitar sesgos en el análisis de texto.\n",
        "def normalize_numbers(text):\n",
        "    # Convertir números a su forma escrita o eliminarlos, según sea necesario\n",
        "    normalized_text = re.sub(r'\\d+', '<número>', text)\n",
        "    return normalized_text\n",
        "\n",
        "# Función para expandir contracciones en español\n",
        "def expand_contractions(text):\n",
        "    contractions_pattern = re.compile(r'\\b(' + '|'.join(CONTRACTIONS.keys()) + r')\\b', re.IGNORECASE)\n",
        "    expanded_text = contractions_pattern.sub(lambda x: CONTRACTIONS[x.group().lower()], text)\n",
        "    return expanded_text\n",
        "\n",
        "# Función para convertir siglas a su forma completa\n",
        "def expand_acronyms(text):\n",
        "    acronyms_pattern = re.compile(r'\\b(' + '|'.join(SIGLAS.keys()) + r')\\b', re.IGNORECASE)\n",
        "    expanded_text = acronyms_pattern.sub(lambda x: SIGLAS[x.group().upper()], text)\n",
        "    return expanded_text\n",
        "\n",
        "# Ejemplo de uso\n",
        "sample_text = \"La NASA y la ONU lanzaron un cohete pa la Luna. La misión tuvo un costo de 3,5 millones de dólares.\"\n",
        "print(\"INPUT: \", sample_text,\"\\n\")\n",
        "\n",
        "# Conversión de siglas\n",
        "noacronyms_text = expand_acronyms(sample_text)\n",
        "print(\"Conversión de siglas:\", noacronyms_text, \"\\n\")\n",
        "\n",
        "# Conversión a minúsculas\n",
        "lowercase_text = to_lowercase(noacronyms_text)\n",
        "print(\"Conversión a minúsculas:\", lowercase_text, \"\\n\")\n",
        "\n",
        "# Expansión de contracciones\n",
        "expanded_text = expand_contractions(lowercase_text)\n",
        "print(\"Expansión de contracciones:\", expanded_text, \"\\n\")\n",
        "\n",
        "# Normalización de números\n",
        "normalized_text = normalize_numbers(expanded_text)\n",
        "print(\"Normalización de números:\", normalized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-chTFZTCz4ef"
      },
      "source": [
        "**Manejo de Emoticones:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsTZKO1kwU3B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ],
      "source": [
        "# %pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HPtW-eb5wP9r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT:  Voy al mercado pal almuerzo. Estoy tan feliz hoy! 😄 No puedo creerlo, aunque mi amigo está triste 😢. \n",
            "\n",
            "Traducción de emoticones: Voy al mercado pal almuerzo. Estoy tan feliz hoy! :grinning_face_with_smiling_eyes: No puedo creerlo, aunque mi amigo está triste :crying_face:. \n",
            "\n",
            "Eliminación de emoticones no relevantes: Voy al mercado pal almuerzo. Estoy tan feliz hoy!  No puedo creerlo, aunque mi amigo está triste .\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from emoji import demojize\n",
        "\n",
        "\n",
        "# Función para traducir emoticones a texto\n",
        "def translate_emoticons(text):\n",
        "    text = demojize(text)  # Convertir emoticones a sus descripciones textuales\n",
        "    emoticon_translation = {\n",
        "        \":sonriente:\": \"feliz\",\n",
        "        \":triste:\": \"triste\",\n",
        "        \":corazón:\": \"amor\",\n",
        "        \":pulgar_hacia_arriba:\": \"aprobación\",\n",
        "        \":pulgar_hacia_abajo:\": \"desaprobación\"\n",
        "    }\n",
        "    for emoticon, meaning in emoticon_translation.items():\n",
        "        text = text.replace(emoticon, meaning)\n",
        "    return text\n",
        "\n",
        "# Función para eliminar emoticones no relevantes\n",
        "def remove_irrelevant_emoticons(text):\n",
        "    text = demojize(text)\n",
        "    text = re.sub(r':[^:]+:', '', text)  # Eliminar todas las descripciones de emoticones\n",
        "    return text\n",
        "\n",
        "# Ejemplo de uso\n",
        "sample_text = \"Voy al mercado pal almuerzo. Estoy tan feliz hoy! 😄 No puedo creerlo, aunque mi amigo está triste 😢.\"\n",
        "print(\"INPUT: \", sample_text,\"\\n\")\n",
        "\n",
        "# Traducción de emoticones\n",
        "translated_text = translate_emoticons(sample_text)\n",
        "print(\"Traducción de emoticones:\", translated_text, \"\\n\")\n",
        "\n",
        "# Eliminación de emoticones no relevantes (si es necesario)\n",
        "clean_text = remove_irrelevant_emoticons(translated_text)\n",
        "print(\"Eliminación de emoticones no relevantes:\", clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhGhNjpx1hAm"
      },
      "source": [
        "**Limpieza de Texto:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulsZruxXN2IF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
            "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
          ]
        }
      ],
      "source": [
        "# %pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2c7UFBCey0s0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT:  \n",
            "Hola,, esttoy aprendiendo procesamiento de lenguaje natural.    Interesánte, eh!! \n",
            "\n",
            "Texto sin puntuación: \n",
            "Hola esttoy aprendiendo procesamiento de lenguaje natural    Interesánte eh \n",
            "\n",
            "Texto sin stop words: Hola esttoy aprendiendo procesamiento lenguaje natural Interesánte eh \n",
            "\n",
            "Texto limpio final: Hola esttoy aprendiendo procesamiento lenguaje natural Interesánte eh\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar las stop words en español\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Inicialización\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "# Función para eliminar puntuación\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Función para eliminar stop words\n",
        "def remove_stop_words(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Función para eliminar espacios en blanco adicionales\n",
        "def remove_extra_whitespace(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Ejemplo de uso\n",
        "sample_text = \"\\nHola,, esttoy aprendiendo procesamiento de lenguaje natural.    Interesánte, eh!!\"\n",
        "print(\"INPUT: \", sample_text,\"\\n\")\n",
        "\n",
        "# 1. Eliminación de Puntuación\n",
        "text_no_punct = remove_punctuation(sample_text)\n",
        "print(\"Texto sin puntuación:\", text_no_punct, \"\\n\")\n",
        "\n",
        "# 2. Eliminación de Stop Words\n",
        "text_no_stop_words = remove_stop_words(text_no_punct)\n",
        "print(\"Texto sin stop words:\", text_no_stop_words, \"\\n\")\n",
        "\n",
        "# 3. Eliminación de Espacios en Blanco Adicionales\n",
        "clean_text = remove_extra_whitespace(text_no_stop_words)\n",
        "print(\"Texto limpio final:\", clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa8w31jusQ0p"
      },
      "source": [
        "**ADICIONAL**: investigar librerías para corrección ortográfica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e_3Q0GxeeQj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n",
            "Texto con corrección ortográfica: Hola esto aprendiente procesamiento lenguaje natural Interesánte eh\n"
          ]
        }
      ],
      "source": [
        "# %pip install pyspellchecker\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker(language='es')\n",
        "\n",
        "def correct_spelling(text):\n",
        "  corrected_text = []\n",
        "  misspelled = spell.unknown(text.split())\n",
        "  for word in text.split():\n",
        "    if word in misspelled:\n",
        "      corrected_word = spell.correction(word)\n",
        "      corrected_text.append(corrected_word)\n",
        "    else:\n",
        "      corrected_text.append(word)\n",
        "  return \" \".join(corrected_text)\n",
        "\n",
        "corrected_text = correct_spelling(clean_text)\n",
        "print(\"Texto con corrección ortográfica:\", corrected_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z56FTcbzhcp"
      },
      "source": [
        "## Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTY57-JB5_8U"
      },
      "source": [
        "**Librería NLTK:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ev6NoevCNPPE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import NLTK and download required data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "74b7eJ74NvzJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Definición de NLTK:\n",
            "\n",
            "NTLK (Natural Language Toolkit) es una biblioteca de procesamiento de lenguaje natural en Python.\n",
            "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenización, análisis gramatical,\n",
            "etiquetado de partes del discurso, entre otros.\n",
            "\n",
            "\n",
            "Ejemplo de Tokenización:\n",
            "Oración original: Apple está buscando comprar una startup del Reino Unido por mil millones de dólares.\n",
            "Tokens: ['Apple', 'está', 'buscando', 'comprar', 'una', 'startup', 'del', 'Reino', 'Unido', 'por', 'mil', 'millones', 'de', 'dólares', '.']\n"
          ]
        }
      ],
      "source": [
        "# Define NLTK\n",
        "nltk_definition = \"\"\"\n",
        "NTLK (Natural Language Toolkit) es una biblioteca de procesamiento de lenguaje natural en Python.\n",
        "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenización, análisis gramatical,\n",
        "etiquetado de partes del discurso, entre otros.\n",
        "\"\"\"\n",
        "\n",
        "# Print definition\n",
        "print(\"\\nDefinición de NLTK:\")\n",
        "print(nltk_definition)\n",
        "\n",
        "# Tokenization example\n",
        "sentence = \"Apple está buscando comprar una startup del Reino Unido por mil millones de dólares.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nEjemplo de Tokenización:\")\n",
        "print(f\"Oración original: {sentence}\")\n",
        "print(f\"Tokens: {tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fDzQRIXDjILw"
      },
      "outputs": [],
      "source": [
        "cuento = \"En una noche tormentosa, Clara escuchó pasos en el ático, aunque vivía sola. \\\n",
        "  Subió las escaleras temblorosa, y al abrir la puerta, encontró una muñeca antigua mirándola fijamente. \\\n",
        "  De repente, la muñeca sonrió y susurró: 'Te estaba esperando.'\"\n",
        "\n",
        "# EJERCICIO: tokenizar palabras, frases, mostrar cantidad de tokens de ambas categorías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSXREfIVfi4"
      },
      "source": [
        "**Librería SpaCy:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18z9xliKNItL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
            "Collecting jinja2 (from spacy)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting setuptools (from spacy)\n",
            "  Downloading setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from spacy) (24.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.33.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-extensions>=4.12.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
            "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/31.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
            "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "Downloading numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
            "Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
            "Downloading pydantic_core-2.33.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Downloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (145 kB)\n",
            "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Downloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
            "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-extensions, spacy-loggers, spacy-legacy, shellingham, setuptools, numpy, murmurhash, mdurl, MarkupSafe, idna, cloudpathlib, charset-normalizer, certifi, catalogue, annotated-types, typing-inspection, srsly, smart-open, requests, pydantic-core, preshed, markdown-it-py, marisa-trie, jinja2, blis, rich, pydantic, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
            "Successfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 certifi-2025.1.31 charset-normalizer-3.4.1 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 idna-3.10 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 numpy-2.2.4 preshed-3.0.9 pydantic-2.11.1 pydantic-core-2.33.0 requests-2.32.3 rich-13.9.4 setuptools-78.1.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 typing-extensions-4.13.0 typing-inspection-0.4.0 urllib3-2.3.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
          ]
        }
      ],
      "source": [
        "# %pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNTlRtt5Vh1M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "# Descarga un modelo de lenguaje para el español en la biblioteca de PLN SpaCy,\n",
        "# optimizado para noticias (\"core_news\") y es de tamaño pequeño (\"sm\" para \"small\")\n",
        "# !python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HkE0_zUaVhse"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Definición de SpaCy:\n",
            "\n",
            "SpaCy es una biblioteca de procesamiento de lenguaje natural en Python, optimizada para rendimiento.\n",
            "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenización, etiquetado de entidades,\n",
            "análisis sintáctico, entre otros.\n",
            "\n",
            "Modelo cargado: core_news_sm\n",
            "\n",
            "Ejemplo de Tokenización con SpaCy:\n",
            "Oración original: Apple está buscando comprar una startup del Reino Unido por mil millones de dólares.\n",
            "Tokens: ['Apple', 'está', 'buscando', 'comprar', 'una', 'startup', 'del', 'Reino', 'Unido', 'por', 'mil', 'millones', 'de', 'dólares', '.']\n",
            "\n",
            "Ventajas de SpaCy:\n",
            "- Rápido: SpaCy está optimizado para un rendimiento rápido, lo que lo hace adecuado para aplicaciones en tiempo real.\n",
            "- Extensible: SpaCy permite la creación de pipelines personalizadas y la integración con otras bibliotecas.\n",
            "- Soporte para múltiples idiomas: SpaCy proporciona modelos para varios idiomas, facilitando el procesamiento de texto multilingüe.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import SpaCy\n",
        "import spacy\n",
        "\n",
        "# Define SpaCy\n",
        "spacy_definition = \"\"\"\n",
        "SpaCy es una biblioteca de procesamiento de lenguaje natural en Python, optimizada para rendimiento.\n",
        "Proporciona herramientas para trabajar con datos textuales, incluyendo tokenización, etiquetado de entidades,\n",
        "análisis sintáctico, entre otros.\n",
        "\"\"\"\n",
        "\n",
        "# Print SpaCy definition\n",
        "print(\"\\nDefinición de SpaCy:\")\n",
        "print(spacy_definition)\n",
        "\n",
        "# model loading\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "print(\"Modelo cargado:\", nlp.meta[\"name\"])\n",
        "\n",
        "# Frase de prueba\n",
        "doc = nlp(\"Apple está buscando comprar una startup del Reino Unido por mil millones de dólares.\")\n",
        "\n",
        "# Print SpaCy results\n",
        "print(\"\\nEjemplo de Tokenización con SpaCy:\")\n",
        "print(f\"Oración original: {doc.text}\")\n",
        "print(\"Tokens:\", [token.text for token in doc])\n",
        "\n",
        "# Advantages of SpaCy\n",
        "spacy_advantages = \"\"\"\n",
        "Ventajas de SpaCy:\n",
        "- Rápido: SpaCy está optimizado para un rendimiento rápido, lo que lo hace adecuado para aplicaciones en tiempo real.\n",
        "- Extensible: SpaCy permite la creación de pipelines personalizadas y la integración con otras bibliotecas.\n",
        "- Soporte para múltiples idiomas: SpaCy proporciona modelos para varios idiomas, facilitando el procesamiento de texto multilingüe.\n",
        "\"\"\"\n",
        "print(spacy_advantages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ei-dtmvHjwv"
      },
      "source": [
        "## Lematización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntv42MrM8hn4"
      },
      "source": [
        "El lema es la forma base o raíz de una palabra. Por ejemplo, \"comprar\" es el lema de \"comprando\".\n",
        "\n",
        "La lematización permite normalizar diferentes formas de una palabra a su forma básica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qNnEXY419DVj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabra: corriendo, Lema: correr\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Cargar el modelo en español de spaCy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto = \"corriendo\"\n",
        "\n",
        "# Procesar el texto con spaCy\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Extraer y mostrar el lema\n",
        "for token in doc:\n",
        "    print(f\"Palabra: {token.text}, Lema: {token.lemma_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLASRVG4B8w0"
      },
      "source": [
        "## Radicalización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JLXDK33-F1XA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras original: ['corriendo', 'corredor', 'corrí', 'cocinero', 'cocina', 'cocinada', 'nacionalidad']\n",
            "Radicalización Porter  : ['corriendo', 'corredor', 'corrí', 'cocinero', 'cocina', 'cocinada', 'nacionalidad']\n",
            "Radicalización Snowball: ['corr', 'corredor', 'corr', 'cociner', 'cocin', 'cocin', 'nacional']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Ejemplos de palabras para radicalizar\n",
        "words = ['corriendo', 'corredor', 'corrí', 'cocinero', 'cocina', 'cocinada', 'nacionalidad']\n",
        "\n",
        "# Porter Stemmer\n",
        "stemmer = PorterStemmer() # only English!!!\n",
        "# Aplicar el Porter Stemmer\n",
        "stemmed_words_porter = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Instanciar el Snowball Stemmer para español\n",
        "stemmer = SnowballStemmer('spanish') # Snowball: extensión del Porter para multilenguaje\n",
        "# Aplicar el Snowball Stemmer\n",
        "stemmed_words_snowball = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Palabras original:\", words)\n",
        "print(\"Radicalización Porter  :\", stemmed_words_porter)\n",
        "print(\"Radicalización Snowball:\", stemmed_words_snowball)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gtRVf1HMjc5"
      },
      "source": [
        "¿Qué podemos analizar de estos resultados?\n",
        "\n",
        "Veamos ahora un ejemplo en inglés:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3fuC39BpHd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras original: ['running', 'runner', 'run', 'cooking', 'cookbook', 'cookery', 'generalization']\n",
            "Radicalización Porter  : ['run', 'runner', 'run', 'cook', 'cookbook', 'cookeri', 'gener']\n",
            "Radicalización Snowball: ['run', 'runner', 'run', 'cook', 'cookbook', 'cookeri', 'general']\n"
          ]
        }
      ],
      "source": [
        "# ENGLISH example\n",
        "\n",
        "# Ejemplos de palabras para radicalizar\n",
        "words = ['running', 'runner', 'run', 'cooking', 'cookbook', 'cookery', 'generalization']\n",
        "\n",
        "# Porter Stemmer\n",
        "stemmer = PorterStemmer() # only English!!!\n",
        "# Aplicar el Porter Stemmer\n",
        "stemmed_words_porter = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Instanciar el Snowball Stemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "# Aplicar el Snowball Stemmer\n",
        "stemmed_words_snowball = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Palabras original:\", words)\n",
        "print(\"Radicalización Porter  :\", stemmed_words_porter)\n",
        "print(\"Radicalización Snowball:\", stemmed_words_snowball)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwdR6uu9H02d"
      },
      "source": [
        "## Etiquetado de Partes del Discurso (POS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E9vtAw4B6ea"
      },
      "source": [
        "La etiqueta de parte del discurso (POS) indica la categoría gramatical de la palabra, algunas de las cuales son:\n",
        "- PROPN: Nombre propio.\n",
        "- AUX: Verbo auxiliar.\n",
        "- VERB: Verbo.\n",
        "- DET: Determinante.\n",
        "- NOUN: Sustantivo.\n",
        "- ADP: Preposición.\n",
        "- NUM: Número.\n",
        "- PUNCT: Signo de puntuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SCfgL5weG8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Etiquetado POS del SpaCy:\n",
            "\n",
            "El: DET\n",
            "gato: NOUN\n",
            "está: AUX\n",
            "debajo: ADV\n",
            "de: ADP\n",
            "la: DET\n",
            "mesa: NOUN\n",
            ".: PUNCT\n"
          ]
        }
      ],
      "source": [
        "# 1): usando spaCy\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Cargar el modelo en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Procesar el texto\n",
        "text = \"El gato está debajo de la mesa.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Mostrar las etiquetas de partes del discurso\n",
        "print(\"Etiquetado POS del SpaCy:\\n\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-p2zkE9AwG5P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oraciones tokenizadas:\n",
            "['El gato Felipe está en el tejado.', 'Parece que va a saltar.']\n",
            "\n",
            "Palabras tokenizadas:\n",
            "['El', 'gato', 'Felipe', 'está', 'en', 'el', 'tejado', '.', 'Parece', 'que', 'va', 'a', 'saltar', '.'] \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package cess_esp is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/gonzadzz/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/share/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(palabras,\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Etiquetar partes del discurso\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m tagged = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpalabras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspa\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Change 'tokens' to 'palabras' to use the correct variable\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEtiquetado POS del NLTK:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(tagged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m    275\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.weights = json.load(fin)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/gonzadzz/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/share/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# 2): usando NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import cess_esp\n",
        "\n",
        "# Descargar los datos necesarios\n",
        "nltk.download('cess_esp')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') # modelo pre-entrenado sólo para English!!!\n",
        "\n",
        "# TEXTO\n",
        "text = \"El gato Felipe está en el tejado. Parece que va a saltar.\"\n",
        "\n",
        "# Tokenización de oraciones\n",
        "oraciones = sent_tokenize(text, language='spanish')\n",
        "print(\"Oraciones tokenizadas:\")\n",
        "print(oraciones)\n",
        "\n",
        "# Tokenización de palabras\n",
        "palabras = word_tokenize(text, language='spanish') # The tokenized words are stored in the variable 'palabras'\n",
        "print(\"\\nPalabras tokenizadas:\")\n",
        "print(palabras,\"\\n\")\n",
        "\n",
        "# Etiquetar partes del discurso\n",
        "tagged = nltk.pos_tag(palabras, lang='spa') # Change 'tokens' to 'palabras' to use the correct variable\n",
        "print(\"Etiquetado POS del NLTK:\\n\")\n",
        "print(tagged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfth2RvGwCbB"
      },
      "source": [
        "Podemos ver que el español no está soportado todavía...\n",
        "\n",
        "Probemos otra aproximación: entrenar un etiquetador POS en español"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UFTwU0o3v_zh"
      },
      "outputs": [],
      "source": [
        "# Cargar el corpus de oraciones etiquetadas\n",
        "oraciones = cess_esp.tagged_sents()\n",
        "\n",
        "# Entrenar un etiquetador basado en unigramas y bigramas\n",
        "from nltk.tag import UnigramTagger, BigramTagger\n",
        "\n",
        "unigram_tagger = UnigramTagger(oraciones)\n",
        "bigram_tagger = BigramTagger(oraciones, backoff=unigram_tagger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fwe-iiFlkhPV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')], [('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explicó', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcción', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prevé', 'vmm02s0'), ('la', 'da0fs0'), ('utilización', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')], ...]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oraciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ESpXwIPSkp-E"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6030"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(oraciones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "X-TegOezxDT7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('El', 'da0ms0'), ('gato', 'ncms000'), ('Felipe', 'np0000p'), ('está', 'vmip3s0'), ('en', 'sps00'), ('el', 'da0ms0'), ('tejado', None), ('.', 'Fp'), ('Parece', 'vmip3s0'), ('que', 'cs'), ('va', 'vmip3s0'), ('a', 'sps00'), ('saltar', 'vmn0000'), ('.', 'Fp')]\n"
          ]
        }
      ],
      "source": [
        "# Prueba del etiquetador\n",
        "text = \"El gato Felipe está en el tejado. Parece que va a saltar.\"\n",
        "tokens = nltk.word_tokenize(text, language='spanish')\n",
        "etiquetas = bigram_tagger.tag(tokens)\n",
        "print(etiquetas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DEXrpXnxYYN"
      },
      "source": [
        "El corpus cess_esp contiene oraciones en español con sus respectivas etiquetas POS.\n",
        "\n",
        "Las etiquetas morfosintácticas están en el formato EAGLES (Expert Advisory Group on Language Engineering Standards), un estándar ampliamente usado en lingüística computacional para la anotación gramatical.\n",
        "\n",
        "Cada etiqueta es un código alfanumérico que describe la categoría gramatical de la palabra y algunas de sus propiedades morfosintácticas:\n",
        "\n",
        "Primera letra: Categoría gramatical principal.\n",
        "\n",
        "- n = Sustantivo (noun)\n",
        "- v = Verbo (verb)\n",
        "- a = Adjetivo (adjective)\n",
        "- d = Determinante (determiner)\n",
        "- s = Preposición o contracción (preposition)\n",
        "- c = Conjunción (conjunction)\n",
        "- p = Pronombre (pronoun)\n",
        "- f = Signo de puntuación (punctuation)\n",
        "\n",
        "Segunda letra: Tipo o subtipo de la categoría.\n",
        "\n",
        "- Para sustantivos (n), indica si es común (c) o propio (p).\n",
        "- Para verbos (v), se refiere al modo (indicativo, subjuntivo, etc.).\n",
        "\n",
        "Tercera letra y siguientes:\n",
        "- Características adicionales como género, número, tiempo verbal, etc.\n",
        "\n",
        "\n",
        "**Consideraciones**:\n",
        "\n",
        "El etiquetador entrenado con el corpus CESS-ESP puede no ser tan preciso para textos fuera de su dominio.\n",
        "\n",
        "Para un etiquetado POS más robusto en español, se recomiendan herramientas como spaCy o Stanza, que tienen modelos entrenados específicamente para español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LBxwYCorHDFP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Kubernetes', None), ('sirve', 'vmip3s0'), ('para', 'sps00'), ('automatizar', None), ('la', 'da0fs0'), ('orquestación', None), ('de', 'sps00'), ('contenedores', 'ncmp000'), ('.', 'Fp')]\n"
          ]
        }
      ],
      "source": [
        "# Frase fuera del contexto de entrenamiento\n",
        "text = \"Kubernetes sirve para automatizar la orquestación de contenedores.\"\n",
        "tokens = nltk.word_tokenize(text, language='spanish')\n",
        "etiquetas = bigram_tagger.tag(tokens)\n",
        "print(etiquetas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udydAe4qH8-_"
      },
      "source": [
        "Veamos un ejemplo en inglés:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FYktpq3LIFcg"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/gonzadzz/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/share/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m tokens = word_tokenize(text, language=\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Mostrar las etiquetas de partes del discurso usando Penn Treebank\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tagged = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meng\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(tagged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/tag/perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m    275\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.weights = json.load(fin)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/gonzadzz/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/share/nltk_data'\n    - '/home/gonzadzz/GitHub/dl_istea/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "text = \"Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services.\"\n",
        "tokens = word_tokenize(text, language='english')\n",
        "\n",
        "# Mostrar las etiquetas de partes del discurso usando Penn Treebank\n",
        "tagged = nltk.pos_tag(tokens, lang='eng')\n",
        "print(tagged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_lwtibFIiMA"
      },
      "source": [
        "Vemos que los códigos son diferentes. Aquí el NLTK usa el conjunto Penn Treebank, que es uno de los más comunes para etiquetado POS en inglés.\n",
        "\n",
        "Explicación de algunos códigos:\n",
        "- NNS: Sustantivo en plural (e.g., \"workloads\", \"services\").\n",
        "- VBZ: Verbo en tercera persona del singular en presente (e.g., \"is\").\n",
        "- DT: Determinante o artículo (e.g., \"a\").\n",
        "- JJ: Adjetivo (e.g., \"portable\", \"extensible\", \"containerized\", \"open\").\n",
        "- ,: Signo de puntuación (coma).\n",
        "- NN: Sustantivo en singular (e.g., \"source\", \"platform\").\n",
        "- IN: Preposición o conjunción subordinante (e.g., \"for\").\n",
        "- VBG: Verbo en gerundio o participio presente (e.g., \"managing\").\n",
        "- CC: Conjunción de coordinación (e.g., \"and\").\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAOrs-pEHEu8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textBlob\n",
            "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: nltk>=3.9 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from textBlob) (3.9.1)\n",
            "Requirement already satisfied: click in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from nltk>=3.9->textBlob) (4.67.1)\n",
            "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: textBlob\n",
            "Successfully installed textBlob-0.19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/gonzadzz/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /home/gonzadzz/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextBlob POS Tagging:\n",
            "El: NNP\n",
            "gato: NN\n",
            "está: NN\n",
            "debajo: NN\n",
            "de: IN\n",
            "la: FW\n",
            "mesa: FW\n"
          ]
        }
      ],
      "source": [
        "# 3): usando TextBlob\n",
        "# %pip install textBlob\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob.download_corpora import download_all\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "download_all()\n",
        "\n",
        "# Crear un objeto TextBlob\n",
        "text = \"El gato está debajo de la mesa.\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Etiquetar partes del discurso\n",
        "print(\"TextBlob POS Tagging:\")\n",
        "for word, tag in blob.tags:\n",
        "    print(f\"{word}: {tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1slgD-UqNyia"
      },
      "source": [
        "¿Qué tal el resultado...?\n",
        "\n",
        "Veamos ahora un ejemplo en inglés:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SULbMJQ91qkx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextBlob POS Tagging:\n",
            "The: DT\n",
            "cat: NN\n",
            "is: VBZ\n",
            "under: IN\n",
            "the: DT\n",
            "table: NN\n"
          ]
        }
      ],
      "source": [
        "# English example\n",
        "\n",
        "# Tokenizar el texto\n",
        "text = \"The cat is under the table.\"\n",
        "\n",
        "# Mostrar las etiquetas de partes del discurso\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Etiquetar partes del discurso\n",
        "print(\"TextBlob POS Tagging:\")\n",
        "for word, tag in blob.tags:\n",
        "    print(f\"{word}: {tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yEL-6zeHNkp"
      },
      "source": [
        "NOTAS:\n",
        "\n",
        "- NLTK: Ofrece flexibilidad y permite personalizar modelos, pero requiere configuraciones adicionales y puede no ser tan preciso en el español.\n",
        "\n",
        "- SpaCy: Proporciona modelos preentrenados precisos y fáciles de usar para múltiples idiomas.\n",
        "\n",
        "- TextBlob: Es una opción más sencilla y ligera, pero con soporte limitado para idiomas distintos del inglés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldjReL1-TsDW"
      },
      "source": [
        "#### Un poco más de información!\n",
        "\n",
        "*Dependencia (Dependency Parsing)*:\n",
        "\n",
        "La dependencia describe la relación sintáctica entre la palabra y su palabra \"padre\" (head). Las etiquetas de dependencia indican el papel gramatical de la palabra en la oración. Algunos ejemplos comunes son:\n",
        "- nsubj: Sujeto nominal.\n",
        "- aux: Auxiliar.\n",
        "- ROOT: Raíz de la oración.\n",
        "- xcomp: Complemento de objeto abierto.\n",
        "- det: Determinante.\n",
        "- obj: Objeto.\n",
        "- case: Preposición o palabra relacionada con la marca de caso.\n",
        "- nmod: Modificador nominal.\n",
        "- flat: Dependencia plana (usualmente para nombres propios compuestos).\n",
        "- nummod: Modificador numérico.\n",
        "- obl: Objeto oblicuo.\n",
        "- nmod: Modificador nominal.\n",
        "- punct: Puntuación.\n",
        "\n",
        "*Padre (Head)*:\n",
        "\n",
        "La palabra padre es el token principal al que está relacionado un token dado en la estructura sintáctica de la oración. En un árbol de dependencias, cada palabra está conectada a otra palabra (su padre) hasta llegar a la raíz de la oración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "s9Fr7js6UMml"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Análisis Sintáctico:\n",
            "Texto: El, Lema: el, POS: DET, Dependencia: det, Padre: gato\n",
            "Texto: gato, Lema: gato, POS: NOUN, Dependencia: nsubj, Padre: debajo\n",
            "Texto: está, Lema: estar, POS: AUX, Dependencia: cop, Padre: debajo\n",
            "Texto: debajo, Lema: debajo, POS: ADV, Dependencia: ROOT, Padre: debajo\n",
            "Texto: de, Lema: de, POS: ADP, Dependencia: case, Padre: mesa\n",
            "Texto: la, Lema: el, POS: DET, Dependencia: det, Padre: mesa\n",
            "Texto: mesa, Lema: mesa, POS: NOUN, Dependencia: obl, Padre: debajo\n",
            "Texto: ., Lema: ., POS: PUNCT, Dependencia: punct, Padre: debajo\n"
          ]
        }
      ],
      "source": [
        "# imprimir el análisis sintáctico, una entidad por línea\n",
        "print(\"\\nAnálisis Sintáctico:\")\n",
        "for token in doc:\n",
        "    print(f\"Texto: {token.text}, Lema: {token.lemma_}, POS: {token.pos_}, Dependencia: {token.dep_}, Padre: {token.head.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "kNzNO5mvxmIe"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m displacy\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Visualizar el análisis sintáctico\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdep\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfont\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAreal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdistance\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcompact\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjupyter\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/spacy/displacy/__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)"
          ]
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# Visualizar el análisis sintáctico\n",
        "displacy.render(doc,style = 'dep', options = {'font':'Areal','distance':100,'compact' : True,}, jupyter =True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou8yQK7HLVNx"
      },
      "source": [
        "## Reconocimiento de entidades nombradas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_71JPNlp1pYW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting es-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.8.0/es_core_news_lg-3.8.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-lg\n",
            "Successfully installed es-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n"
          ]
        }
      ],
      "source": [
        "# Modelo de lenguaje para el español en la biblioteca de PLN SpaCy,\n",
        "# optimizado para noticias (\"core_news\") y es de tamaño grande (\"lg\")\n",
        "!python -m spacy download es_core_news_lg #md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ1EP2J5Yp59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (24.2)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
            "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Installing collected packages: safetensors, pyyaml, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed filelock-3.18.0 fsspec-2025.3.0 huggingface-hub-0.29.3 pyyaml-6.0.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entidades Nombradas con SpaCy --español--:\n",
            "[('Raúl Alfonsín', 'PER'), ('Buenos Aires', 'LOC'), ('Argentina', 'LOC')]\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEntidades Nombradas con SpaCy --español--:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(entidades_spacy)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_spacy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m texto = \u001b[33m\"\u001b[39m\u001b[33mRaúl Alfonsín, a lawyer born in Buenos Aires, was the President of Argentina between 1983 and 1989.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m nlp_spacy = spacy.load(\u001b[33m\"\u001b[39m\u001b[33men_core_web_sm\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/dl_istea/.venv/lib/python3.12/site-packages/spacy/displacy/__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/home/gonzadzz/GitHub/dl_istea/.venv/lib/python3.12/site-packages/IPython/core/display.py)"
          ]
        }
      ],
      "source": [
        "%pip install transformers \n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from textblob import TextBlob\n",
        "from transformers import pipeline\n",
        "\n",
        "# Cargar el modelo de SpaCy para español\n",
        "nlp_spacy = spacy.load(\"es_core_news_lg\")\n",
        "\n",
        "# Texto a analizar\n",
        "texto = \"Raúl Alfonsín, abogado nacido en Buenos Aires, fue presidente de Argentina, entre 1983 y 1989.\"\n",
        "\n",
        "# 1. Reconocimiento de Entidades Nombradas con SpaCy\n",
        "doc_spacy = nlp_spacy(texto)\n",
        "entidades_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents]\n",
        "print(\"\\nEntidades Nombradas con SpaCy --español--:\")\n",
        "print(entidades_spacy)\n",
        "displacy.render(doc_spacy, style=\"ent\")\n",
        "\n",
        "\n",
        "texto = \"Raúl Alfonsín, a lawyer born in Buenos Aires, was the President of Argentina between 1983 and 1989.\"\n",
        "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "doc_spacy = nlp_spacy(texto)\n",
        "entidades_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents]\n",
        "print(\"\\nEntidades Nombradas con SpaCy --inglés--:\")\n",
        "print(entidades_spacy)\n",
        "displacy.render(doc_spacy, style=\"ent\")\n",
        "\n",
        "\n",
        "# 2. Reconocimiento de Entidades Nombradas con TextBlob\n",
        "# TextBlob no tiene un modelo preentrenado para NER en español, por lo que no es adecuado para este propósito\n",
        "# Para demostrar el uso de TextBlob en inglés, el siguiente código sería un ejemplo:\n",
        "blob = TextBlob(\"Raúl Alfonsín, a lawyer born in Buenos Aires, was the President of Argentina between 1983 and 1989.\")\n",
        "entidades_textblob = blob.noun_phrases\n",
        "print(\"\\nEntidades Nombradas con TextBlob (Inglés):\")\n",
        "print(entidades_textblob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVxufJNUSpX9"
      },
      "source": [
        "¿Qué diferencias se pueden apreciar en los resultados?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMXlOMknLwOO"
      },
      "source": [
        "## Ejercicio: Análisis de Texto en Español con NLTK y SpaCy\n",
        "\n",
        "Objetivos:\n",
        "- Aprender a tokenizar texto utilizando NLTK y SpaCy.\n",
        "- Comparar los resultados de tokenización entre ambas librerías.\n",
        "- Utilizar SpaCy para realizar el reconocimiento de entidades nombradas.\n",
        "- Realizar el análisis sintáctico de una oración con SpaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "H6da4WCpMDLF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparación de tokenizaciones:\n",
            "NTLK : ['El', 'presidente', 'Barack', 'Obama', 'visitó', 'París', 'en', 'julio', 'de', '2023', 'para', 'una', 'cumbre', 'internacional', '.']\n",
            "SpaCy: ['El', 'presidente', 'Barack', 'Obama', 'visitó', 'París', 'en', 'julio', 'de', '2023', 'para', 'una', 'cumbre', 'internacional', '.']\n",
            "\n",
            "Entidades Nombradas con SpaCy:\n",
            "[('Barack Obama visitó París', 'PER')]\n",
            "\n",
            "Análisis sintáctico completo con SpaCy:\n",
            "Texto: El, Lema: el, POS: DET, Dependencia: det, Padre: presidente\n",
            "Texto: presidente, Lema: presidente, POS: NOUN, Dependencia: nsubj, Padre: visitó\n",
            "Texto: Barack, Lema: Barack, POS: PROPN, Dependencia: appos, Padre: presidente\n",
            "Texto: Obama, Lema: Obama, POS: PROPN, Dependencia: flat, Padre: Barack\n",
            "Texto: visitó, Lema: visitar, POS: VERB, Dependencia: ROOT, Padre: visitó\n",
            "Texto: París, Lema: París, POS: PROPN, Dependencia: obj, Padre: visitó\n",
            "Texto: en, Lema: en, POS: ADP, Dependencia: case, Padre: julio\n",
            "Texto: julio, Lema: julio, POS: NOUN, Dependencia: obl, Padre: visitó\n",
            "Texto: de, Lema: de, POS: ADP, Dependencia: case, Padre: 2023\n",
            "Texto: 2023, Lema: 2023, POS: NUM, Dependencia: compound, Padre: julio\n",
            "Texto: para, Lema: para, POS: ADP, Dependencia: case, Padre: cumbre\n",
            "Texto: una, Lema: uno, POS: DET, Dependencia: det, Padre: cumbre\n",
            "Texto: cumbre, Lema: cumbre, POS: NOUN, Dependencia: obl, Padre: visitó\n",
            "Texto: internacional, Lema: internacional, POS: ADJ, Dependencia: amod, Padre: cumbre\n",
            "Texto: ., Lema: ., POS: PUNCT, Dependencia: punct, Padre: visitó\n"
          ]
        }
      ],
      "source": [
        "# Texto a analizar\n",
        "texto = \"El presidente Barack Obama visitó París en julio de 2023 para una cumbre internacional.\"\n",
        "#texto = \"El Dr. Juan Pérez, un reconocido neurocirujano de Nueva York, intervino exitosamente a la Sra. Julia González.\"\n",
        "\n",
        "# Tokenización con NLTK\n",
        "tokens_nltk = nltk.word_tokenize(texto)\n",
        "\n",
        "# Tokenización con SpaCy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "doc = nlp(texto)\n",
        "tokens_spacy = [token.text for token in doc]\n",
        "\n",
        "# Comparación de Tokenización\n",
        "print(\"\\nComparación de tokenizaciones:\")\n",
        "print(\"NTLK :\", tokens_nltk)\n",
        "print(\"SpaCy:\", tokens_spacy)\n",
        "\n",
        "\n",
        "# Reconocimiento de Entidades Nombradas con SpaCy\n",
        "entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"\\nEntidades Nombradas con SpaCy:\")\n",
        "print(entidades)\n",
        "\n",
        "# Análisis Sintáctico con SpaCy\n",
        "print(\"\\nAnálisis sintáctico completo con SpaCy:\")\n",
        "for token in doc:\n",
        "    print(f\"Texto: {token.text}, Lema: {token.lemma_}, POS: {token.pos_}, Dependencia: {token.dep_}, Padre: {token.head.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTp-M8gzMGGP"
      },
      "source": [
        "Discusión para los resultados con ambras frases:\n",
        "\n",
        "- ¿Qué diferencias notaste en la tokenización entre NLTK y SpaCy?\n",
        "- ¿Qué ventajas ofrece SpaCy en términos de rendimiento y capacidades adicionales?\n",
        "\n",
        "Discusión general:\n",
        "\n",
        "- ¿Cómo puede el reconocimiento de entidades nombradas ayudar en el análisis de texto?\n",
        "- ¿Qué información útil puedes obtener del análisis sintáctico realizado con SpaCy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcdiSK1WSziR"
      },
      "source": [
        "1) La tokenización de SpaCy es más precisa y maneja mejor los signos de puntuación y las entidades compuestas en comparación con NLTK.\n",
        "\n",
        "2) SpaCy ofrece un rendimiento superior y capacidades adicionales como el reconocimiento de entidades nombradas y el análisis sintáctico, haciéndolo más adecuado para aplicaciones en tiempo real.\n",
        "\n",
        "3) El reconocimiento de entidades nombradas facilita la identificación de elementos clave en el texto, como nombres de personas, lugares y organizaciones, mejorando la comprensión y el análisis contextual.\n",
        "\n",
        "4) El análisis sintáctico con SpaCy proporciona información detallada sobre las relaciones gramaticales entre las palabras, lo que ayuda a entender la estructura y el significado subyacente de las oraciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuQh6_3OmiZC"
      },
      "source": [
        "#### Ejemplo de análisis de sentimiento usando NLTK\n",
        "\n",
        "Fuente:\n",
        "https://www.datacamp.com/es/tutorial/text-analytics-beginners-nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU-Kt3ZImsNu"
      },
      "outputs": [],
      "source": [
        "# Start codiimport pandas as pd \n",
        "%pip install pandas\n",
        "%pip install pandas\n",
        " \n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# download nltk corpus\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "# Load the amazon review dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/amazon.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s83nexRNpUHU"
      },
      "outputs": [],
      "source": [
        "# imprimir 5 filas con columna Positive igual a 0\n",
        "df[df['Positive'] == 0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAFRDwJGnX3Z"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "df['reviewText'] = df['reviewText'].apply(preprocess_text)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtrN3CJmqqxI"
      },
      "outputs": [],
      "source": [
        "# imprimir 5 filas con columna Positive igual a 0\n",
        "df[df['Positive'] == 0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK7SHupLnlvd"
      },
      "outputs": [],
      "source": [
        "# Perform sentiment analysis using NLTK Vader\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    sentiment = 1 if scores['pos'] > 0 else 0\n",
        "    return sentiment\n",
        "\n",
        "df['sentiment'] = df['reviewText'].apply(get_sentiment)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB_LBPCrq51w"
      },
      "outputs": [],
      "source": [
        "# Ejemplo del análisis con 1 frase, para ver la salida del analizador\n",
        "\n",
        "texto = df['reviewText'][1]\n",
        "print(texto)\n",
        "analyzer.polarity_scores(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy_oXItDobku"
      },
      "outputs": [],
      "source": [
        "# imprimir 5 filas de df donde el sentimiento sea 0\n",
        "df[df['sentiment'] == 0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI-wLpxdoLLi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(df['Positive'], df['sentiment']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp-JdBVeoRHS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df['Positive'], df['sentiment']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5DIigZuZPEG"
      },
      "source": [
        "## ANEXO\n",
        "\n",
        "La librería SpaCy contiene un conjunto de frases ya disponibles para pruebas, accesibles a través de la estructura de datos 'sentences':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxZIFTT-g_cY"
      },
      "outputs": [],
      "source": [
        "# Frases de ejemplo disponibles en SpaCy:\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.es.examples import sentences\n",
        "\n",
        "# imprimir el contenido de sentences, una frase por línea\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "doc = nlp(sentences[0])\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    #print(token.text, token.pos_, token.dep_)\n",
        "    print(f\"Texto: {token.text}, Lema: {token.lemma_}, POS: {token.pos_}, Dependencia: {token.dep_}, Padre: {token.head.text}\")\n",
        "entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"\\nEntidades nombradas:\")\n",
        "print(entidades)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDTNBiiUZwyM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-5DIigZuZPEG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
